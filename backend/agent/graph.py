# backend/agent/graph.py
import os
import json
import asyncio
import itertools
from typing import Literal, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# [Import] ë¡œì»¬ ëª¨ë“ˆ
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision, 
    ResearchActionPlan,
    SearchStrategyPlan,
    ResearcherOutput,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
)

from .prompts import (
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_CHAT_PROMPT,
    WRITER_RECOMMENDATION_PROMPT,
    NOTE_SELECTION_PROMPT,
)
from .database import save_recommendation_log

# [ì •ë³´ ê²€ìƒ‰ ì „ìš© ì„œë¸Œ ê·¸ë˜í”„ ì„í¬íŠ¸]
from .graph_info import info_graph

load_dotenv()

# ==========================================
# 1. ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4.1", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)

# ==========================================
# 2. ìœ í‹¸ë¦¬í‹°
# ==========================================
def log_filters(h_filters: dict, s_filters: dict):
    h_items = [f"{k.capitalize()}: {v}" for k, v in h_filters.items() if v]
    h_str = " | ".join(h_items) if h_items else "None"

    s_items = []
    for k, v in s_filters.items():
        if v:
            val_str = str(v) if not isinstance(v, list) else f"{v}"
            s_items.append(f"{k.capitalize()}: {val_str}")
    s_str = " | ".join(s_items) if s_items else "None"

    print(f"       ğŸ”’ [Hard] {h_str}", flush=True)
    print(f"       âœ¨ [Soft] {s_str}", flush=True)

async def smart_search_with_retry_async(
    h_filters: dict, s_filters: dict, exclude_ids: list = None, query_text: str = ""
):
    priority_order = ["note", "accord", "occasion"]
    active_keys = [k for k in priority_order if k in s_filters and s_filters[k]]

    results = await advanced_perfume_search_tool.ainvoke(
        {
            "hard_filters": h_filters,
            "strategy_filters": s_filters,
            "exclude_ids": exclude_ids,
            "query_text": query_text,
        }
    )
    if results:
        return results, "Perfect Match"

    for r in range(len(active_keys) - 1, 0, -1):
        for combo_keys in itertools.combinations(active_keys, r):
            temp_filters = {k: s_filters[k] for k in combo_keys}
            results = await advanced_perfume_search_tool.ainvoke(
                {
                    "hard_filters": h_filters,
                    "strategy_filters": temp_filters,
                    "exclude_ids": exclude_ids,
                    "query_text": query_text,
                }
            )
            if results:
                return results, f"Relaxed (Level {len(active_keys)-r})"
    return [], "No Results"

async def call_info_graph_wrapper(state: AgentState):
    """Sub-Graph Wrapper"""
    print(f"\nğŸš€ [Main Graph] 'info_graph' ì„œë¸Œ ê·¸ë˜í”„ í˜¸ì¶œ...", flush=True)
    current_query = state.get("user_query", "")
    
    if not current_query and state.get("messages"):
        last_msg = state["messages"][-1]
        if isinstance(last_msg, HumanMessage):
            current_query = last_msg.content
            
    print(f"   ğŸ‘‰ ì „ë‹¬í•  Query: {current_query}", flush=True)

    subgraph_input = {
        "user_query": current_query,
        "messages": state.get("messages", [])
    }
    
    try:
        result = await info_graph.ainvoke(subgraph_input)
        print(f"âœ… [Main Graph] ì„œë¸Œ ê·¸ë˜í”„ ì™„ë£Œ. ê²°ê³¼ ë³µê·€.", flush=True)
        return {"messages": result.get("messages", [])}
        
    except Exception as e:
        print(f"ğŸš¨ [Main Graph] ì„œë¸Œ ê·¸ë˜í”„ ì—ëŸ¬: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return {"messages": [AIMessage(content="ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")]}

# ==========================================
# 3. Node Functions
# ==========================================

def supervisor_node(state: AgentState):
    """[Main Router]"""
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜ ì¤‘...", flush=True)
    
    if state.get("active_mode") == "interviewer":
        print("   ğŸ‘‰ ì¸í„°ë·° ì§„í–‰ ì¤‘ -> Interviewerë¡œ ì´ë™", flush=True)
        return {"next_step": "interviewer"}

    messages = [SystemMessage(content=SUPERVISOR_PROMPT)] + state["messages"]
    
    try:
        decision = SMART_LLM.with_structured_output(RoutingDecision).invoke(messages)
        next_step = decision.next_step 
        print(f"   ğŸ‘‰ ë¶„ë¥˜ ê²°ê³¼: {next_step}", flush=True)
        return {"next_step": next_step}
        
    except Exception as e:
        print(f"   âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨(Error): {e} -> ê¸°ë³¸ê°’ Writerë¡œ ì´ë™", flush=True)
        return {"next_step": "writer"}


def interviewer_node(state: AgentState):
    """[Interviewer]"""
    print(f"\nğŸ¤ [Interviewer] ì¶”ì²œ ì •ë³´ ë¶„ì„ ë° ê²€ì¦...", flush=True)
    current_prefs = state.get("user_preferences", {})
    messages = [SystemMessage(content=INTERVIEWER_PROMPT)] + state["messages"]
    
    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        new_prefs = result.user_preferences.dict(exclude_unset=True)
        updated_prefs = {
            **current_prefs,
            **{k: v for k, v in new_prefs.items() if v is not None},
        }

        if result.is_sufficient:
            print(f"      âœ… [Handover] ì •ë³´ í™•ë³´ ì™„ë£Œ! Researcherë¡œ ì „ë‹¬: {json.dumps(updated_prefs, ensure_ascii=False)}", flush=True)
            return {
                "next_step": "researcher",
                "user_preferences": updated_prefs,
                "status": "ëª¨ë“  ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤...",
                "active_mode": None, 
            }
            
        return {
            "messages": [AIMessage(content=result.response_message)],
            "user_preferences": updated_prefs,
            "active_mode": "interviewer", 
            "next_step": "end", 
        }
    except Exception as e:
        print(f"Interviewer Error: {e}")
        return {"next_step": "writer"} 


async def researcher_node(state: AgentState):
    print(f"\nğŸ§  [Researcher] ì „ëµ ìˆ˜ë¦½ ë° ë³‘ë ¬ DB ê²€ìƒ‰ ì‹œì‘...", flush=True)
    current_member_id = state.get("member_id", 0)
    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)

    user_note = user_prefs.get("note")
    refined_hard_note = None
    if user_note:
        matched = await lookup_note_by_string_tool.ainvoke({"keyword": user_note})
        if matched:
            refined_hard_note = matched[0]

    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(
            content=f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {current_context}\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì´ë¯¸ì§€ ê°•ì¡°, ë³´ì™„, ë°˜ì „'ì˜ 3ê°€ì§€ ê²€ìƒ‰ ì „ëµì„ ì„¸ì›Œì£¼ì„¸ìš”."
        ),
    ]
    plan_result = await SMART_LLM.with_structured_output(ResearchActionPlan).ainvoke(messages)

    async def process_strategy_candidates(plan: SearchStrategyPlan):
        print(f"   ğŸ‘‰ [Parallel Task] {plan.strategy_name}", flush=True)
        h_filters = plan.hard_filters.model_dump(exclude_none=True)
        if refined_hard_note:
            h_filters["note"] = refined_hard_note
        s_filters = plan.strategy_filters.model_dump(exclude_none=True)

        strategy_note_input = s_filters.get("note")
        if strategy_note_input:
            raw_keyword = (
                strategy_note_input[0]
                if isinstance(strategy_note_input, list)
                else strategy_note_input
            )
            candidates = await lookup_note_by_vector_tool.ainvoke({"keyword": raw_keyword})
            if candidates:
                selection_messages = [
                    SystemMessage(content=NOTE_SELECTION_PROMPT.format(candidates=candidates)),
                    HumanMessage(content=f"ì „ëµ: {plan.strategy_name}\nì˜ë„: {plan.reason}"),
                ]
                selected_res = await SMART_LLM.ainvoke(selection_messages)
                llm_selected = [c for c in candidates if c.lower() in selected_res.content.lower()]
                s_filters["note"] = llm_selected if llm_selected else candidates[:1]

        log_filters(h_filters, s_filters)
        db_perfumes, match_type = await smart_search_with_retry_async(
            h_filters, s_filters, query_text=plan.reason
        )
        return {"plan": plan, "candidates": db_perfumes}

    tasks = [process_strategy_candidates(p) for p in plan_result.plans]
    all_candidates_results = await asyncio.gather(*tasks)

    final_results = []
    seen_perfume_ids = set()

    for item in all_candidates_results:
        plan = item["plan"]
        candidates = item["candidates"]
        
        selected_p = None
        for p in candidates:
            if p["id"] not in seen_perfume_ids:
                selected_p = p
                seen_perfume_ids.add(p["id"])
                break
        
        if not selected_p:
            # [â˜…ìˆ˜ì •: ë¡œê·¸ ì¶”ê°€] ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
            print(f"      âŒ [Result] {plan.strategy_name}: ê²€ìƒ‰ëœ í–¥ìˆ˜ê°€ ì—†ê±°ë‚˜ ì¤‘ë³µë˜ì–´ ì„ íƒ ì‹¤íŒ¨", flush=True)
            continue

        # [â˜…ìˆ˜ì •: ë¡œê·¸ ì¶”ê°€] ê²€ìƒ‰ ì„±ê³µ ì‹œ í–¥ìˆ˜ ì´ë¦„ ì¶œë ¥
        print(f"      âœ… [Result] {plan.strategy_name}: {selected_p.get('brand')} - {selected_p.get('name')} (ID: {selected_p.get('id')})", flush=True)

        save_recommendation_log(
            member_id=current_member_id, perfumes=[selected_p], reason=plan.reason
        )

        final_results.append(
            StrategyResult(
                strategy_name=plan.strategy_name,
                strategy_keyword=plan.strategy_keyword,
                strategy_reason=plan.reason,
                perfumes=[
                    PerfumeDetail(
                        id=selected_p.get("id"),
                        perfume_name=selected_p.get("name"),
                        perfume_brand=selected_p.get("brand"),
                        accord=f"{selected_p.get('accords')}\n[Best Review]: {selected_p.get('best_review')}",
                        notes=PerfumeNotes(
                            top=selected_p.get("top_notes") or "N/A",
                            middle=selected_p.get("middle_notes") or "N/A",
                            base=selected_p.get("base_notes") or "N/A",
                        ),
                        image_url=selected_p.get("image_url"),
                        gender=selected_p.get("gender", "Unisex"),
                        season="All",
                        occasion="Any",
                    )
                ],
            )
        )

    if not final_results:
        print(f"      âŒ [Result] ëª¨ë“  ì „ëµì—ì„œ ì¶”ì²œ ê°€ëŠ¥í•œ í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", flush=True)
        return {
            "research_results": {"results": []},
            # [ì¤‘ìš”] ì‹¤íŒ¨í–ˆìŒì„ ëª…ì‹œí•˜ëŠ” ë©”ì‹œì§€ë¡œ ë³€ê²½
            "messages": [AIMessage(content="[RESEARCH_FAILED]")], 
            "next_step": "writer",
            # [ì¤‘ìš”] ìƒíƒœ ë©”ì‹œì§€ ìˆ˜ì • -> ì´ë ‡ê²Œ í•´ì•¼ Writerê°€ ì‚¬ê³¼ ë©˜íŠ¸ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.
            "status": "ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜¢ ëŒ€ì•ˆì„ ì•ˆë‚´í•´ ë“œë¦´ê²Œìš”...",
        }

    # ì„±ê³µí–ˆì„ ë•Œ (ê¸°ì¡´ ë¡œì§)
    return {
        "research_results": {"results": [r.dict() for r in final_results]},
        "messages": [AIMessage(content="[RESEARCH_DONE]")],
        "next_step": "writer",
        "status": "ì „ëµì— ë§ëŠ” í–¥ìˆ˜ë“¤ì„ ëª¨ë‘ ì°¾ì•˜ìŠµë‹ˆë‹¤! ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤...",
    }


async def writer_node(state: AgentState):
    print(f"\nâœï¸ [Writer] ë‹µë³€ ì‘ì„± ì¤‘...", flush=True)
    research_data = state.get("research_results", {})
    results_list = research_data.get("results", [])

    prompt = WRITER_RECOMMENDATION_PROMPT if results_list else WRITER_FAILURE_PROMPT
    if not results_list and state.get("next_step") == "writer":
        prompt = WRITER_CHAT_PROMPT

    data_ctx = (
        json.dumps(research_data, ensure_ascii=False, indent=2) if results_list else ""
    )
    messages = [
        SystemMessage(content=f"{prompt}\n\n[ì°¸ê³  ë°ì´í„°]:\n{data_ctx}")
    ] + state["messages"]

    response = await SUPER_SMART_LLM.ainvoke(messages)
    return {"messages": [response], "next_step": "end"}


# ==========================================
# 4. Graph Build
# ==========================================
workflow = StateGraph(AgentState)

workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("writer", writer_node)
workflow.add_node("info_retrieval_subgraph", call_info_graph_wrapper)

workflow.add_edge(START, "supervisor")

workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {
        "interviewer": "interviewer",
        "info_retrieval": "info_retrieval_subgraph",
        "writer": "writer"
    },
)

workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {
        "end": END,                
        "researcher": "researcher", 
        "writer": "writer"          
    },
)

workflow.add_edge("researcher", "writer")
workflow.add_edge("writer", END)
workflow.add_edge("info_retrieval_subgraph", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)