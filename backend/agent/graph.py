import os
import json
import traceback
from typing import Literal, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# [Import] ë¡œì»¬ ëª¨ë“ˆ - schemas.pyì—ì„œ ì •ì˜í•œ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision,
    ResearchActionPlan,
    SearchStrategyPlan,
    HardFilters,
    StrategyFilters,
    ResearcherOutput,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
)

from .prompts import (
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_CHAT_PROMPT,
    WRITER_RECOMMENDATION_PROMPT,
    NOTE_SELECTION_PROMPT,
)

load_dotenv()

# ==========================================
# 1. ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)


# ==========================================
# 3. Node Functions
# ==========================================
def supervisor_node(state: AgentState):
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ëŒ€í™” ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ ì¤‘...", flush=True)

    # 1. ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (íœ˜ë°œ ë°©ì§€)
    current_prefs = state.get("user_preferences", {})

    # 2. ì´ë¯¸ ì¸í„°ë·° ëª¨ë“œë¼ë©´ ë°”ë¡œ ì¸í„°ë·°ì–´ë¡œ í† ìŠ¤
    if state.get("active_mode") == "interviewer":
        print("   -> â© Active Mode: Interviewer ìœ ì§€", flush=True)
        return {"next_step": "interviewer"}

    # 3. ì¸í„°ë·°ì–´ì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
    # Supervisor ë‹¨ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ Researcherê°€ ë¹ˆ ê°’ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.
    messages = [SystemMessage(content=INTERVIEWER_PROMPT)] + state["messages"]

    try:
        # ì •ë°€í•œ ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ SMART_LLM(gpt-4o)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)

        # 4. ì •ë³´ ì—…ë°ì´íŠ¸ ë° ë³‘í•©
        new_prefs = result.user_preferences.dict(exclude_unset=True)
        updated_prefs = {
            **current_prefs,
            **{k: v for k, v in new_prefs.items() if v is not None},
        }

        # 5. ë¼ìš°íŒ… íŒë‹¨
        # [Case A] í–¥ìˆ˜ì™€ ê´€ë ¨ ì—†ëŠ” ì¡ë‹´ì¸ ê²½ìš°
        if result.is_off_topic:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: WRITER (Off-topic)", flush=True)
            return {"next_step": "writer", "active_mode": None}

        # [Case B] í•„ìˆ˜ ì •ë³´(Target + Concept)ê°€ ì¶©ì¡±ëœ ê²½ìš° -> ë°”ë¡œ ê²€ìƒ‰
        if result.is_sufficient:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: RESEARCHER (ì •ë³´ ì¶©ì¡±)", flush=True)
            print(
                f"      ìˆ˜ì§‘ ì •ë³´: {json.dumps(updated_prefs, ensure_ascii=False)}",
                flush=True,
            )
            return {
                "next_step": "researcher",
                "user_preferences": updated_prefs,
                "active_mode": None,
            }

        # [Case C] ì •ë³´ê°€ ë” í•„ìš”í•œ ê²½ìš° -> Interviewerì—ê²Œ ì „ë‹¬
        else:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: INTERVIEWER (ì¶”ê°€ ì§ˆë¬¸ í•„ìš”)", flush=True)
            return {
                "next_step": "interviewer",
                "user_preferences": updated_prefs,
                "active_mode": "interviewer",
            }

    except Exception as e:
        print(f"   -> âš ï¸ Supervisor Error: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì¸í„°ë·°ì–´ ë‹¨ê³„ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        return {"next_step": "interviewer"}


# ==========================================
# 4. Interviewer ë…¸ë“œ ì •ì˜
# ==========================================


def interviewer_node(state: AgentState):
    print(f"\nğŸ¤ [Interviewer] ì •ë³´ ë¶„ì„ ì¤‘...", flush=True)
    current_prefs = state.get("user_preferences", {})
    current_prefs_str = (
        json.dumps(current_prefs, ensure_ascii=False, indent=2)
        if current_prefs
        else "ì—†ìŒ"
    )

    augmented_prompt = f"""
    {INTERVIEWER_PROMPT}
    [â˜…Contextâ˜…] ì´ì „ ìˆ˜ì§‘ ì •ë³´: {current_prefs_str}
    """
    messages = [SystemMessage(content=augmented_prompt)] + state["messages"]

    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        print(
            f"   -> ğŸ“Š íŒë‹¨: ì¶©ì¡±({result.is_sufficient}), ì¡ë‹´({result.is_off_topic})",
            flush=True,
        )

        if result.is_off_topic:
            return {"active_mode": None, "next_step": "writer"}

        if result.is_sufficient:
            print("   -> ğŸš€ ì •ë³´ ì¶©ì¡±! Researcher í˜¸ì¶œ", flush=True)
            return {
                "messages": [AIMessage(content=result.response_message)],
                "user_preferences": result.user_preferences.dict(),
                "active_mode": None,
                "next_step": "researcher",
            }
        else:
            print("   -> â“ ì •ë³´ ë¶€ì¡±", flush=True)
            print("í˜„ì¬ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì •ë³´ :", result.user_preferences.dict())
            return {
                "messages": [AIMessage(content=result.response_message)],
                "user_preferences": result.user_preferences.dict(),
                "active_mode": "interviewer",
                "next_step": "end",
            }
    except Exception as e:
        print(f"   -> âš ï¸ Error: {e}")
        return {"active_mode": None, "next_step": "writer"}


# ==========================================
# 5. Researcherì— ì‚¬ìš©ë  ê¸°ëŠ¥í•¨ìˆ˜ ì •ì˜
# ==========================================
def log_filters(h_filters: dict, s_filters: dict):
    # [ìˆ˜ì •] Hard Filtersì— ë‹´ê¸´ ëª¨ë“  ìš”ì†Œë¥¼ ë™ì ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
    hard_items = []
    for k, v in h_filters.items():
        if v:
            hard_items.append(f"{k.capitalize()}: {v}")

    hard_str = f"ğŸ”’ [Hard] " + (" | ".join(hard_items) if hard_items else "None")

    # Soft Filter í¬ë§¤íŒ… (ê¸°ì¡´ ìœ ì§€)
    soft_items = []
    for k, v in s_filters.items():
        if v:
            soft_items.append(f"{k.capitalize()}: {v}")

    soft_str = f"âœ¨ [Soft] " + (" | ".join(soft_items) if soft_items else "None")

    print(f"      {hard_str}", flush=True)
    print(f"      {soft_str}", flush=True)


def smart_search_with_retry(
    h_filters: dict,
    s_filters: dict,
    exclude_ids: list = None,
    query_text: str = "",  # [ì¶”ê°€] ë¦¬ë­í‚¹ì„ ìœ„í•œ ì „ëµ ì´ìœ (Reason)
):
    import copy

    # ì›ë³¸ ë³´ì¡´ì„ ìœ„í•´ ë”¥ì¹´í”¼ ì‚¬ìš©
    current_filters = copy.deepcopy(s_filters)

    # 1. Full Condition ì‹œë„
    print(f"\n      ğŸ“ [Attempt 1] Full Conditions", flush=True)
    # log_filters(h_filters, current_filters) # ë¡œê·¸ í•¨ìˆ˜ê°€ ìˆë‹¤ë©´ ìœ ì§€

    # [ìˆ˜ì •] advanced_perfume_search_tool ì‚¬ìš© (1ì°¨ ê²€ìƒ‰ -> ë¦¬ë·° ë¦¬ë­í‚¹ -> Top 5)
    results = advanced_perfume_search_tool.invoke(
        {
            "hard_filters": h_filters,
            "strategy_filters": current_filters,
            "exclude_ids": exclude_ids,
            "query_text": query_text,  # [ì¶”ê°€] ì „ëµ ì´ìœ  ì „ë‹¬
        }
    )

    if results:
        print(
            f"      âœ… Found {len(results)} perfumes (Perfect Match + Reranked)",
            flush=True,
        )
        return results, "Perfect Match"

    # 2. Waterfall (ë‹¨ê³„ì  ì¡°ê±´ ì™„í™”)
    drop_priority = ["occasion", "style", "accord", "note"]

    for i, key in enumerate(drop_priority):
        if key in current_filters and current_filters[key]:
            dropped_val = current_filters[key]
            del current_filters[key]

            print(
                f"\n      ğŸ“ [Attempt {i+2}] Relaxing... (Drop {key.upper()}: {dropped_val})",
                flush=True,
            )

            # [ìˆ˜ì •] advanced_perfume_search_tool ì‚¬ìš©
            results = advanced_perfume_search_tool.invoke(
                {
                    "hard_filters": h_filters,
                    "strategy_filters": current_filters,
                    "exclude_ids": exclude_ids,
                    "query_text": query_text,  # [ì¶”ê°€] ì „ëµ ì´ìœ  ì „ë‹¬
                }
            )

            if results:
                match_type = f"Relaxed (Dropped {key})"
                print(
                    f"      âœ… Found {len(results)} perfumes ({match_type})", flush=True
                )
                return results, match_type

    return [], "No Results"


# ==========================================
# 6. Researcherë…¸ë“œ ì •ì˜
# ==========================================
def researcher_node(state: AgentState):
    print(f"\nğŸ§  [Researcher] ì „ëµ ìˆ˜ë¦½ ë° DB ê²€ìƒ‰...", flush=True)

    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)
    print(f"   ğŸ‘¤ User Context: {current_context}", flush=True)

    # [1] Hard Filterìš© ë…¸íŠ¸ ì „ì²˜ë¦¬
    user_note = user_prefs.get("note")
    refined_hard_note = None
    if user_note:
        matched_notes = lookup_note_by_string_tool.invoke({"keyword": user_note})
        if matched_notes:
            refined_hard_note = matched_notes[0]
            print(
                f"   ğŸ¯ User Note Refined: '{user_note}' -> '{refined_hard_note}'",
                flush=True,
            )

    # [2] ì „ëµ ìˆ˜ë¦½ ë©”ì‹œì§€ ìƒì„± (ê¸°ì¡´ ìœ ì§€)
    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(
            content=f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {current_context}\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì´ë¯¸ì§€ ê°•ì¡°, ë³´ì™„, ë°˜ì „'ì˜ 3ê°€ì§€ ê²€ìƒ‰ ì „ëµì„ ì„¸ì›Œì£¼ì„¸ìš”."
        ),
    ]

    try:
        plan_result = SMART_LLM.with_structured_output(ResearchActionPlan).invoke(
            messages
        )
        final_results = []
        collected_ids = []

        for plan in plan_result.plans:
            print(f"\n   " + "-" * 50, flush=True)
            print(f"   ğŸ‘‰ [Strategy {plan.priority}] {plan.strategy_name}", flush=True)

            current_reason = plan.reason

            # [ì¶”ê°€] ë¦¬ë­í‚¹ìš© ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„± (ì´ìœ  + í‚¤ì›Œë“œ)
            search_query_text = (
                f"{current_reason}. Keywords: {', '.join(plan.strategy_keyword)}"
            )

            h_filters = (
                plan.hard_filters.model_dump(exclude_none=True)
                if hasattr(plan.hard_filters, "model_dump")
                else plan.hard_filters.dict(exclude_none=True)
            )

            # [ì•ˆì „ì¥ì¹˜] ë§¤í•‘ (ê¸°ì¡´ ìœ ì§€)
            if h_filters.get("season") == "ë´„":
                h_filters["season"] = "Spring"
            if h_filters.get("gender") == "ë‚¨ì„±":
                h_filters["gender"] = "Men"
            if refined_hard_note:
                h_filters["note"] = refined_hard_note

            s_filters = (
                plan.strategy_filters.model_dump(exclude_none=True)
                if hasattr(plan.strategy_filters, "model_dump")
                else plan.strategy_filters.dict(exclude_none=True)
            )

            # [3] Strategy Filterìš© ë…¸íŠ¸ í›„ë³´êµ° ì¶”ì¶œ (ê¸°ì¡´ ìœ ì§€)
            strategy_note_input = s_filters.get("note")
            if strategy_note_input:
                raw_keyword = (
                    strategy_note_input[0]
                    if isinstance(strategy_note_input, list) and strategy_note_input
                    else strategy_note_input
                )

                if raw_keyword:
                    print(
                        f"      ğŸ” '{raw_keyword}' ê¸°ë°˜ ë…¸íŠ¸ í›„ë³´êµ° ì¶”ì¶œ ì¤‘...",
                        flush=True,
                    )
                    candidates = lookup_note_by_vector_tool.invoke(
                        {"keyword": raw_keyword}
                    )

                    if candidates:
                        print(f"      â¡ï¸ ì¶”ì¶œëœ í›„ë³´êµ°: {candidates}", flush=True)
                        selection_messages = [
                            SystemMessage(
                                content=NOTE_SELECTION_PROMPT.format(
                                    candidates=candidates
                                )
                            ),
                            HumanMessage(
                                content=f"í˜„ì¬ ì „ëµ: {plan.strategy_name}\nì˜ë„: {current_reason}"
                            ),
                        ]
                        selected_response = SMART_LLM.invoke(selection_messages).content
                        final_selected = [
                            c
                            for c in candidates
                            if c.lower() in selected_response.lower()
                        ]

                        s_filters["note"] = (
                            final_selected if final_selected else candidates[:1]
                        )
                        print(
                            f"      ğŸ¯ LLM ìµœì¢… ì„ íƒ ë…¸íŠ¸: {s_filters['note']}",
                            flush=True,
                        )

            # [4] ê²€ìƒ‰ ìˆ˜í–‰ (ë¦¬ë­í‚¹ ì ìš©)
            # [ìˆ˜ì •] query_text ì¸ì ì¶”ê°€
            db_perfumes, match_type = smart_search_with_retry(
                h_filters,
                s_filters,
                exclude_ids=collected_ids,
                query_text=search_query_text,
            )

            # [5] ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ Re-Act (ê¸°ì¡´ ë¡œì§ + query_text ì¶”ê°€)
            if not db_perfumes:
                print(
                    f"      âš ï¸ '{plan.strategy_name}' ê²°ê³¼ ì—†ìŒ. ì¬ìˆ˜ë¦½ ì‹œë„...",
                    flush=True,
                )
                retry_messages = [
                    SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
                    HumanMessage(
                        content=f"ì‚¬ìš©ì ì •ë³´: {current_context}\nì‹¤íŒ¨í•œ í•„í„°: {json.dumps(s_filters)}\nì „ëµì— ë¶€í•©í•˜ëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œì™€ ì‚¬ìœ (Reason)ë¥¼ ì œì•ˆí•´ì¤˜."
                    ),
                ]
                new_plan = SMART_LLM.with_structured_output(SearchStrategyPlan).invoke(
                    retry_messages
                )

                s_filters = (
                    new_plan.strategy_filters.model_dump(exclude_none=True)
                    if hasattr(new_plan.strategy_filters, "model_dump")
                    else new_plan.strategy_filters.dict(exclude_none=True)
                )
                current_reason = new_plan.reason
                search_query_text = f"{current_reason}. Keywords: {', '.join(new_plan.strategy_keyword)}"  # ì¿¼ë¦¬ ì—…ë°ì´íŠ¸

                # ë…¸íŠ¸ ì¬ê²€ìƒ‰ ë¡œì§ (ê¸°ì¡´ ìœ ì§€)
                if s_filters.get("note"):
                    retry_keyword = (
                        s_filters["note"][0]
                        if isinstance(s_filters["note"], list)
                        else s_filters["note"]
                    )
                    retry_candidates = lookup_note_by_vector_tool.invoke(
                        {"keyword": retry_keyword}
                    )
                    if retry_candidates:
                        s_filters["note"] = retry_candidates[:2]

                # [ìˆ˜ì •] ì¬ì‹œë„ í˜¸ì¶œ ì‹œì—ë„ query_text ì „ë‹¬
                db_perfumes, match_type = smart_search_with_retry(
                    h_filters,
                    s_filters,
                    exclude_ids=collected_ids,
                    query_text=search_query_text,
                )

            # [6] ê²°ê³¼ ì •ë¦¬
            perfume_details = []
            if db_perfumes:
                p = db_perfumes[0]
                collected_ids.append(p["id"])
                print(
                    f"      âœ… ìµœì¢… ì„ ì •: {p.get('brand')} - {p.get('name')} ({match_type})",
                    flush=True,
                )

                # [ì¶”ê°€] ë² ìŠ¤íŠ¸ ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                best_review_text = p.get("best_review", "ë¦¬ë·° ì •ë³´ ì—†ìŒ")

                # Writerì—ê²Œ ì „ë‹¬í•  Accord ì •ë³´ì— ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ì„œ í’ë¶€í•œ ë§¥ë½ ì œê³µ
                accord_with_review = f"{p.get('accords') or 'ì •ë³´ ì—†ìŒ'}\n[âœ¨ Best Review]: {best_review_text}"

                p_notes = PerfumeNotes(
                    top=p.get("top_notes") or "ì •ë³´ ì—†ìŒ",
                    middle=p.get("middle_notes") or "ì •ë³´ ì—†ìŒ",
                    base=p.get("base_notes") or "ì •ë³´ ì—†ìŒ",
                )
                detail = PerfumeDetail(
                    perfume_name=p.get("name", "Unknown"),
                    perfume_brand=p.get("brand", "Unknown"),
                    accord=accord_with_review,  # [ìˆ˜ì •] ë¦¬ë·° í¬í•¨ëœ Accord ì „ë‹¬
                    season="All Seasons",
                    occasion="Any",
                    gender=p.get("gender", "Unisex"),
                    notes=p_notes,
                    image_url=p.get("image_url"),
                )
                perfume_details.append(detail)

            final_results.append(
                StrategyResult(
                    strategy_name=plan.strategy_name,
                    strategy_keyword=plan.strategy_keyword,
                    strategy_reason=current_reason,
                    perfumes=perfume_details,
                )
            )

        return {
            "research_results": (
                ResearcherOutput(results=final_results).model_dump()
                if hasattr(ResearcherOutput, "model_dump")
                else ResearcherOutput(results=final_results).dict()
            ),
            "messages": [AIMessage(content="[RESEARCH_DONE]")],
            "next_step": "writer",
        }

    except Exception as e:
        print(f"   -> ğŸš¨ Researcher Node Error: {e}")
        import traceback

        traceback.print_exc()
        return {"research_results": {"results": []}, "next_step": "writer"}


# ==========================================
# 7. Writerë…¸ë“œ ì •ì˜ (ë¹„ë™ê¸° ì²˜ë¦¬ ì ìš©)
# ==========================================


async def writer_node(state: AgentState):
    print(f"\nâœï¸ [Writer] ìµœì¢… ë‹µë³€ ì‘ì„± ì¤‘...", flush=True)
    last_message = state["messages"][-1]
    research_data = state.get("research_results", {})
    results_list = research_data.get("results", [])

    if isinstance(last_message, HumanMessage):
        selected_prompt = WRITER_CHAT_PROMPT
        data_context = ""
    elif not results_list or all(len(r["perfumes"]) == 0 for r in results_list):
        selected_prompt = WRITER_FAILURE_PROMPT
        data_context = ""
    else:
        selected_prompt = WRITER_RECOMMENDATION_PROMPT
        data_context = json.dumps(research_data, ensure_ascii=False, indent=2)

    full_content = f"{selected_prompt}\n\n[ì°¸ê³  ë°ì´í„°]:\n{data_context}"
    messages = [SystemMessage(content=full_content)] + state["messages"]

    try:
        # ainvokeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
        # astream_eventsê°€ ì´ ë‚´ë¶€ì˜ ìŠ¤íŠ¸ë¦¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
        response = await SUPER_SMART_LLM.ainvoke(messages)
        return {"messages": [response], "next_step": "end"}
    except Exception:
        return {"next_step": "end"}


# 4. Graph Build
workflow = StateGraph(AgentState)
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("writer", writer_node)

workflow.add_edge(START, "supervisor")
workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {"interviewer": "interviewer", "researcher": "researcher", "writer": "writer"},
)
workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {"end": END, "researcher": "researcher", "writer": "writer"},
)
workflow.add_edge("researcher", "writer")
workflow.add_edge("writer", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
