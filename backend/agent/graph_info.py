# backend/agent/graph_info.py
import json
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END

# [1] ìŠ¤í‚¤ë§ˆ ì„í¬íŠ¸
from .schemas_info import InfoState, InfoRoutingDecision
from .tools_schemas_info import IngredientAnalysisResult

# [2] ë„êµ¬ ì„í¬íŠ¸
from .tools_info import (
    lookup_perfume_info_tool,
    lookup_perfume_by_id_tool,
    lookup_note_info_tool,
    lookup_accord_info_tool,
)
from .tools_similarity import lookup_similar_perfumes_tool

# [3] í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
from .prompts_info import (
    INFO_SUPERVISOR_PROMPT,
    PERFUME_DESCRIBER_PROMPT_BEGINNER,
    PERFUME_DESCRIBER_PROMPT_EXPERT,
    SIMILARITY_CURATOR_PROMPT_BEGINNER,
    SIMILARITY_CURATOR_PROMPT_EXPERT,
    INGREDIENT_SPECIALIST_PROMPT,
)

# [4] Expression Loader for dynamic dictionary injection
from .expression_loader import ExpressionLoader

load_dotenv()

# [LLM ì´ì›í™”]
INFO_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
ROUTER_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=False)


# ==========================================
# 4. Utility Functions for Ordinal/Pronoun Resolution
# ==========================================

import re
from typing import List, Dict, Optional


def extract_save_refs(messages: List) -> List[Dict[str, any]]:
    """
    Extract SAVE tags from most recent AIMessage containing recommendations.
    Returns list of {id: int, name: str} in order of appearance.
    """
    save_pattern = re.compile(r'\[\[SAVE:(\d+):([^\]]+)\]\]')
    
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content:
            matches = save_pattern.findall(msg.content)
            if matches:
                return [{"id": int(m[0]), "name": m[1]} for m in matches]
    
    return []


def parse_ordinal(user_query: str) -> Optional[int]:
    """
    Parse ordinal numbers from Korean text (supports 1-10).
    Returns 1-based index (1, 2, 3, ...) or None if not found.
    """
    query_lower = user_query.lower()
    
    numeric_match = re.search(r'(\d+)\s*(ë²ˆì§¸|ë²ˆ)\b', query_lower)
    if numeric_match:
        return int(numeric_match.group(1))
    
    korean_ordinals = {
        'ì²«': 1, 'ì²«ë²ˆì§¸': 1, '1ë²ˆì§¸': 1, '1ë²ˆ': 1,
        'ë‘': 2, 'ë‘ë²ˆì§¸': 2, 'ë‘˜ì§¸': 2, '2ë²ˆì§¸': 2, '2ë²ˆ': 2,
        'ì„¸': 3, 'ì„¸ë²ˆì§¸': 3, 'ì…‹ì§¸': 3, '3ë²ˆì§¸': 3, '3ë²ˆ': 3,
        'ë„¤': 4, 'ë„¤ë²ˆì§¸': 4, 'ë„·ì§¸': 4, '4ë²ˆì§¸': 4, '4ë²ˆ': 4,
        'ë‹¤ì„¯': 5, 'ë‹¤ì„¯ë²ˆì§¸': 5, 'ë‹¤ì„¯ì§¸': 5, '5ë²ˆì§¸': 5, '5ë²ˆ': 5,
        'ì—¬ì„¯': 6, 'ì—¬ì„¯ë²ˆì§¸': 6, 'ì—¬ì„¯ì§¸': 6, '6ë²ˆì§¸': 6, '6ë²ˆ': 6,
        'ì¼ê³±': 7, 'ì¼ê³±ë²ˆì§¸': 7, 'ì¼ê³±ì§¸': 7, '7ë²ˆì§¸': 7, '7ë²ˆ': 7,
        'ì—¬ëŸ': 8, 'ì—¬ëŸë²ˆì§¸': 8, 'ì—¬ëŸì§¸': 8, '8ë²ˆì§¸': 8, '8ë²ˆ': 8,
        'ì•„í™‰': 9, 'ì•„í™‰ë²ˆì§¸': 9, 'ì•„í™‰ì§¸': 9, '9ë²ˆì§¸': 9, '9ë²ˆ': 9,
        'ì—´': 10, 'ì—´ë²ˆì§¸': 10, 'ì—´ì§¸': 10, '10ë²ˆì§¸': 10, '10ë²ˆ': 10,
    }
    
    for pattern, num in korean_ordinals.items():
        if pattern in query_lower:
            return num
    
    return None


def resolve_target_from_ordinal_or_pronoun(
    user_query: str,
    router_target_name: str,
    save_refs: List[Dict[str, any]]
) -> Optional[Dict[str, any]]:
    """
    Resolve target perfume from ordinal numbers or pronouns.
    Returns {id: int, name: str} or None if resolution fails.
    """
    pronouns = ['ì´ê±°', 'ê·¸ê±°', 'ì´ í–¥ìˆ˜', 'ì €ê±°']
    generic_terms = ['ì¶”ì²œí•´ì¤˜', 'ë¹„ìŠ·í•œê±°']
    
    ordinal = parse_ordinal(user_query)
    is_pronoun = any(p in user_query for p in pronouns)
    is_generic = router_target_name in generic_terms or any(g in router_target_name for g in generic_terms)
    
    if ordinal:
        if 1 <= ordinal <= len(save_refs):
            return save_refs[ordinal - 1]
        else:
            return None
    
    if is_pronoun or is_generic:
        if save_refs:
            return save_refs[-1]
    
    return None


# ==========================================
# 5. Streaming Helper for Silent Failure Prevention
# ==========================================

async def stream_fixed_message(text: str) -> AIMessage:
    """
    Stream a fixed message through LLM to ensure output appears in UI.
    Prevents silent failures by guaranteeing on_chat_model_stream events.
    """
    system_prompt = "Output EXACTLY the next user message. Do not add, remove, or change any character. No quotes."
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=text)
    ]
    
    response = await INFO_LLM.ainvoke(messages)
    
    if response.content.strip() != text.strip():
        print(f"      âš ï¸ [Stream Mismatch] Expected: '{text}' | Got: '{response.content}'", flush=True)
        
        retry_system = "Your previous output was invalid. Output the next user message EXACTLY, character-for-character."
        retry_messages = [
            SystemMessage(content=retry_system),
            HumanMessage(content=text)
        ]
        response = await INFO_LLM.ainvoke(retry_messages)
        
        if response.content.strip() != text.strip():
            print(f"      âš ï¸ [Stream Retry Failed] Using retry output anyway", flush=True)
    
    return response


# ==========================================
# 6. Node Functions
# ==========================================


def info_supervisor_node(state: InfoState):
    """[Router] ë¶„ë¥˜ ë…¸ë“œ"""
    print(f"\n   â–¶ï¸ [Info Subgraph] Supervisor ë…¸ë“œ ì‹œì‘", flush=True)
    user_query = state.get("user_query", "")

    chat_history = state.get("messages", [])
    context_str = ""
    if chat_history:
        recent_msgs = chat_history[-3:] if len(chat_history) > 3 else chat_history
        for msg in recent_msgs:
            role = "User" if isinstance(msg, HumanMessage) else "AI"
            if msg.content:
                context_str += f"- {role}: {msg.content}\n"

    final_system_prompt = INFO_SUPERVISOR_PROMPT
    if context_str:
        final_system_prompt += f"\n\n[Recent Chat Context]\n{context_str}"

    final_system_prompt += "\n\n[Instruction]\nResolve the target name from context and classify based on the PRIORITY rules."

    messages = [
        SystemMessage(content=final_system_prompt),
        HumanMessage(content=user_query),
    ]

    try:
        decision = ROUTER_LLM.with_structured_output(InfoRoutingDecision).invoke(
            messages
        )
        final_target = decision.target_name
        
        save_refs = extract_save_refs(chat_history)
        
        resolved = resolve_target_from_ordinal_or_pronoun(
            user_query, final_target, save_refs
        )
        
        if resolved:
            ordinal = parse_ordinal(user_query)
            
            info_type = decision.info_type
            if any(kw in user_query for kw in ['ë¹„ìŠ·', 'ì¶”ì²œ', 'ëŒ€ì²´', 'ê°™ì€']):
                info_type = "similarity"
            elif resolved:
                info_type = "perfume"
            
            return {
                "info_type": info_type,
                "target_id": resolved['id'],
                "target_name": resolved['name']
            }
        
        if not save_refs and (parse_ordinal(user_query) or any(p in user_query for p in ['ì´ê±°', 'ê·¸ê±°', 'ì´ í–¥ìˆ˜', 'ì €ê±°'])):
            fail_msg = "ìµœê·¼ì— ì¶”ì²œë“œë¦° í–¥ìˆ˜ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. í–¥ìˆ˜ ì´ë¦„ì„ ì§ì ‘ ë§ì”€í•´ ì£¼ì‹œë©´ ë°”ë¡œ ì°¾ì•„ë“œë¦´ê²Œìš”."
            return {"info_type": "unknown", "target_name": "unknown", "fail_msg": fail_msg}
        
        ordinal = parse_ordinal(user_query)
        if ordinal and ordinal > len(save_refs):
            fail_msg = f"ì§€ê¸ˆ ì¶”ì²œì€ 1~{len(save_refs)}ë²ˆì§¸ê¹Œì§€ ìˆì–´ìš”. ì›í•˜ì‹œëŠ” ë²ˆí˜¸ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            return {"info_type": "unknown", "target_name": "unknown", "fail_msg": fail_msg}
        
        if not final_target or final_target in [
            "ì´ê±°",
            "ê·¸ê±°",
            "ì´ í–¥ìˆ˜",
            "ì¶”ì²œí•´ì¤˜",
            "ë¹„ìŠ·í•œê±°",
        ]:
            return {"info_type": "unknown", "target_name": "unknown"}

        return {"info_type": decision.info_type, "target_name": final_target}

    except Exception as e:
        print(f"      âŒ Supervisor ì—ëŸ¬ ë°œìƒ: {e}", flush=True)
        return {"info_type": "unknown", "target_name": "unknown"}


async def perfume_describer_node(state: InfoState):
    """[Perfume Expert] ìƒì„¸ ì •ë³´"""
    target = state["target_name"]
    target_id = state.get("target_id")

    user_mode = state.get("user_mode", "BEGINNER")
    try:
        if target_id:
            search_result_json = await lookup_perfume_by_id_tool.ainvoke({"perfume_id": target_id})
        else:
            search_result_json = await lookup_perfume_info_tool.ainvoke(target)

        # [â˜…ìˆ˜ì •] ê°€ë“œë ˆì¼ ê°•í™”: "ê²€ìƒ‰ ì‹¤íŒ¨" ë¿ë§Œ ì•„ë‹ˆë¼ "DB ì—ëŸ¬"ë‚˜ "Error"ê°€ í¬í•¨ëœ ê²½ìš°ë„ ì°¨ë‹¨
        is_error = any(
            keyword in search_result_json
            for keyword in ["ê²€ìƒ‰ ì‹¤íŒ¨", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "DB ì—ëŸ¬", "Error"]
        )
        is_empty = (
            not search_result_json
            or search_result_json == "{}"
            or search_result_json == "[]"
        )

        if is_error or is_empty:
            if target_id and target:
                search_result_json = await lookup_perfume_info_tool.ainvoke(target)
                
                is_error_retry = any(
                    keyword in search_result_json
                    for keyword in ["ê²€ìƒ‰ ì‹¤íŒ¨", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "DB ì—ëŸ¬", "Error"]
                )
                is_empty_retry = (
                    not search_result_json
                    or search_result_json == "{}"
                    or search_result_json == "[]"
                )
                
                if is_error_retry or is_empty_retry:
                    fail_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}'ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜¢"
                    response = await stream_fixed_message(fail_msg)
                    return {"messages": [response], "final_answer": response.content}
            else:
                fail_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}'ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜¢"
                response = await stream_fixed_message(fail_msg)
                return {"messages": [response], "final_answer": response.content}

        if user_mode == "EXPERT":
            print("      ğŸ˜ [Mode] ì „ë¬¸ê°€ìš© ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì ìš©")
            selected_prompt = PERFUME_DESCRIBER_PROMPT_EXPERT
        else:
            print("      ğŸ¥ [Mode] ë¹„ê¸°ë„ˆìš© ë„ìŠ¨íŠ¸ í”„ë¡¬í”„íŠ¸ ì ìš©")
            selected_prompt = PERFUME_DESCRIBER_PROMPT_BEGINNER

        # [â˜… Dynamic Expression Injection]
        # Parse perfume data to extract notes and accords
        try:
            perfume_data = json.loads(search_result_json)
            perfume_name = perfume_data.get("name", "Unknown")
            brand = perfume_data.get("brand", "Unknown")
            
            loader = ExpressionLoader()
            expression_guide = []
            injected_count = 0
            
            all_notes = []
            all_accords = []
            
            # Extract notes
            for note_type in ["top_notes", "middle_notes", "base_notes"]:
                note_str = perfume_data.get(note_type, "")
                if note_str and note_str != "N/A":
                    notes = [n.strip() for n in note_str.split(",")]
                    all_notes.extend(notes)
                    for note in notes[:5]:  # Limit per type
                        desc = loader.get_note_desc(note)
                        if desc:
                            expression_guide.append(f"- {note}: {desc}")
                            injected_count += 1
            
            # Extract accords
            accord_str = perfume_data.get("accords", "")
            if accord_str:
                accords = [a.strip() for a in accord_str.split(",")]
                all_accords = accords
                for accord in accords[:5]:
                    desc = loader.get_accord_desc(accord)
                    if desc:
                        expression_guide.append(f"- {accord}: {desc}")
                        injected_count += 1
            
            expression_text = "\n".join(expression_guide) if expression_guide else ""
            
        except Exception as e:
            expression_text = ""

        content_parts = [f"ëŒ€ìƒ í–¥ìˆ˜: {target}"]
        if expression_text:
            content_parts.append(f"\n[ê°ê° í‘œí˜„ ì°¸ê³ ]:\n{expression_text}")
        content_parts.append(f"\n[ê²€ìƒ‰ëœ ìƒì„¸ ì •ë³´]:\n{search_result_json}")

        messages = [
            SystemMessage(content=selected_prompt),
            HumanMessage(content="\n".join(content_parts)),
        ]
        response = await INFO_LLM.ainvoke(messages)

        return {"messages": [response], "final_answer": response.content}

    except Exception as e:
        print(f"      âŒ Perfume Describer ì—ëŸ¬: {e}", flush=True)
        msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}' ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ê¸°ìˆ ì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        response = await stream_fixed_message(msg)
        return {"messages": [response], "final_answer": response.content}


async def ingredient_specialist_node(state: InfoState):
    """[Ingredient Expert] ì„±ë¶„ ë¶„ì„"""
    try:
        user_query = state.get("user_query", "")
        target_name = state.get("target_name", "")
        print(
            f"\n   â–¶ï¸ [Info Subgraph] Ingredient Specialist: '{user_query}'", flush=True
        )

        # 1. ì¿¼ë¦¬ ë¶„ì„ ë¡œì§ (ì›ë˜ ë¡œì§ ìœ ì§€)
        analysis_prompt = f"""
        You are a query analyzer. Separate 'Notes' and 'Accords'.
        Query: "{user_query}"
        Context Target: "{target_name}"
        Output JSON: {{ "notes": [], "accords": [], "is_ambiguous": false }}
        """

        try:
            analysis = await ROUTER_LLM.with_structured_output(
                IngredientAnalysisResult
            ).ainvoke(analysis_prompt, config={"tags": ["internal_helper"]})
            print(
                f"      - ë¶„ì„ ê²°ê³¼: Notes={analysis.notes}, Accords={analysis.accords}",
                flush=True,
            )
        except Exception as e:
            print(f"      âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}", flush=True)
            analysis = IngredientAnalysisResult(notes=[target_name], accords=[])

        # 2. ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ (ì›ë˜ ë¡œì§ ìœ ì§€)
        tasks = []
        tasks.append(
            lookup_note_info_tool.ainvoke({"keywords": analysis.notes})
            if analysis.notes
            else asyncio.sleep(0, result="")
        )
        tasks.append(
            lookup_accord_info_tool.ainvoke({"keywords": analysis.accords})
            if analysis.accords
            else asyncio.sleep(0, result="")
        )

        results = await asyncio.gather(*tasks)
        note_result, accord_result = results[0], results[1]

        # 3. ìƒì„¸ ë¡œê¹… í•¨ìˆ˜ ë° ì‹¤í–‰ (ì›ë˜ ë¡œì§ ìœ ì§€)
        def print_result_log(category: str, result_str: str):
            if not result_str:
                return
            try:
                data = json.loads(result_str)
                if not data:
                    print(f"      ğŸ” [{category}]: ê²°ê³¼ ì—†ìŒ (Empty)", flush=True)
                    return
                for key, val in data.items():
                    if isinstance(val, dict):
                        perfumes = val.get("representative_perfumes", [])
                        perfume_log = ", ".join(perfumes) if perfumes else "ì—†ìŒ"
                        print(
                            f"      ğŸ” [{category}] '{key}': (ëŒ€í‘œí–¥ìˆ˜: {perfume_log})",
                            flush=True,
                        )
            except:
                pass

        print_result_log("Note DB", note_result)
        print_result_log("Accord DB", accord_result)

        # =============================================================
        # [â˜… í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: ì¡°ê¸° ì°¨ë‹¨(Early Exit) ê°€ë“œë ˆì¼]
        # =============================================================
        # ë¶„ì„ëœ ë…¸íŠ¸ì™€ ì–´ì½”ë“œì— ëŒ€í•´ DB ê²€ìƒ‰ ê²°ê³¼ê°€ ëª¨ë‘ ìœ íš¨í•˜ì§€ ì•Šì€ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        # ê²°ê³¼ê°€ ì—†ê±°ë‚˜("{}"), ê²€ìƒ‰ ì‹¤íŒ¨ ë©”ì‹œì§€ê°€ í¬í•¨ëœ ê²½ìš° LLM í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤.
        is_note_empty = (
            not note_result or "ê²€ìƒ‰ ì‹¤íŒ¨" in note_result or note_result == "{}"
        )
        is_accord_empty = (
            not accord_result or "ê²€ìƒ‰ ì‹¤íŒ¨" in accord_result or accord_result == "{}"
        )

        if is_note_empty and is_accord_empty:
            print(
                f"      âš ï¸ [Hallucination Guard] ë°ì´í„° ë¶€ì¬ë¡œ LLM í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤.",
                flush=True,
            )
            fail_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë§ì”€í•˜ì‹  '{user_query}' ì„±ë¶„ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ê°€ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ì— ë“±ë¡ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ğŸ˜¢"
            response = await stream_fixed_message(fail_msg)
            return {"messages": [response], "final_answer": response.content}
        # =============================================================

        # 4. LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± (ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰)
        # [â˜… Dynamic Expression Injection]
        loader = ExpressionLoader()
        expression_guide = []
        injected_count = 0
        
        # Add note descriptions
        for note in analysis.notes[:10]:
            desc = loader.get_note_desc(note)
            if desc:
                expression_guide.append(f"- {note}: {desc}")
                injected_count += 1
        
        # Add accord descriptions
        for accord in analysis.accords[:10]:
            desc = loader.get_accord_desc(accord)
            if desc:
                expression_guide.append(f"- {accord}: {desc}")
                injected_count += 1
        
        expression_text = "\n".join(expression_guide) if expression_guide else ""
        
        context_parts = [
            f"[User Interest]: Notes: {analysis.notes}, Accords: {analysis.accords}",
        ]
        
        if expression_text:
            context_parts.append(f"\n[ê°ê° í‘œí˜„ ì°¸ê³ ]:\n{expression_text}")
        
        context_parts.append(f"""
        [Search Results]:
        --- Note Data ---
        {note_result}
        --- Accord Data ---
        {accord_result}
        """)
        
        combined_context = "\n".join(context_parts)

        messages = [
            SystemMessage(content=INGREDIENT_SPECIALIST_PROMPT),
            HumanMessage(content=combined_context),
        ]
        response = await INFO_LLM.ainvoke(messages)

        return {"messages": [response], "final_answer": response.content}

    except Exception as e:
        print(f"      âŒ Ingredient Specialist ì—ëŸ¬: {e}", flush=True)
        msg = "ì„±ë¶„ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ë„ì¤‘ ê¸°ìˆ ì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        response = await stream_fixed_message(msg)
        return {"messages": [response], "final_answer": response.content}


async def similarity_curator_node(state: InfoState):
    """[Similarity Expert] ìœ ì‚¬ ì¶”ì²œ"""

    user_mode = state.get("user_mode", "BEGINNER")
    try:
        target = state["target_name"]

        # 1. ë„êµ¬ í˜¸ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        search_result_json = await lookup_similar_perfumes_tool.ainvoke(target)

        # =============================================================
        # [â˜… í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€: ì¡°ê¸° ì°¨ë‹¨(Early Exit) ê°€ë“œë ˆì¼]
        # =============================================================
        # ìœ ì‚¬ í–¥ìˆ˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ë©”ì‹œì§€ì¸ ê²½ìš°, LLM í˜¸ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.
        # ê²°ê³¼ê°€ "[]"ì´ê±°ë‚˜ íŠ¹ì • ì‹¤íŒ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        is_empty = (
            not search_result_json
            or search_result_json == "[]"
            or "{}" in search_result_json
        )
        is_failed = (
            "ê²€ìƒ‰ ì‹¤íŒ¨" in search_result_json or "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in search_result_json
        )

        if is_empty or is_failed:
            print(
                f"      âš ï¸ [Hallucination Guard] ìœ ì‚¬ í–¥ìˆ˜ ë°ì´í„° ë¶€ì¬ë¡œ LLM í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤.",
                flush=True,
            )
            fail_msg = f"í˜„ì¬ ì €í¬ ë°ì´í„°ë² ì´ìŠ¤ì—ëŠ” '{target}'ê³¼ ê²°ì´ ë¹„ìŠ·í•œ í–¥ìˆ˜ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë„¤ìš”. ğŸ˜… ë‹¤ë¥¸ í–¥ìˆ˜ë¡œ ë‹¤ì‹œ ì°¾ì•„ë´ ë“œë¦´ê¹Œìš”?"
            response = await stream_fixed_message(fail_msg)
            return {"messages": [response], "final_answer": response.content}
        # =============================================================
        if user_mode == "EXPERT":
            print("      ğŸ˜ [Mode] ì „ë¬¸ê°€ìš© íë ˆì´í„° í”„ë¡¬í”„íŠ¸ ì ìš©")
            selected_prompt = SIMILARITY_CURATOR_PROMPT_EXPERT
        else:
            print("      ğŸ¥ [Mode] ë¹„ê¸°ë„ˆìš© ë„ìŠ¨íŠ¸ í”„ë¡¬í”„íŠ¸ ì ìš©")
            selected_prompt = SIMILARITY_CURATOR_PROMPT_BEGINNER

        messages = [
            SystemMessage(content=selected_prompt),
            HumanMessage(
                content=f"ì›ë³¸ í–¥ìˆ˜: {target}\n\n[ì¶”ì²œ í›„ë³´êµ° ë°ì´í„°]:\n{search_result_json}"
            ),
        ]
        response = await INFO_LLM.ainvoke(messages)

        # [â˜…ìˆ˜ì •] ê²°ê³¼ê°€ í™”ë©´ì— ë‚˜ì˜¤ë„ë¡ final_answerë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜
        return {"messages": [response], "final_answer": response.content}

    except Exception as e:
        # ì‹œìŠ¤í…œ ì—ëŸ¬ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        print(f"      âŒ Similarity Curator ì—ëŸ¬: {e}", flush=True)
        msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}'ê³¼ ìœ ì‚¬í•œ í–¥ìˆ˜ë¥¼ ì°¾ëŠ” ê³¼ì •ì—ì„œ ê¸°ìˆ ì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        response = await stream_fixed_message(msg)
        return {"messages": [response], "final_answer": response.content}


async def fallback_handler_node(state: InfoState):
    """[Fallback] ì•ˆë‚´"""
    print(f"\n   âš ï¸ [Info Subgraph] Fallback Handler ì‹¤í–‰", flush=True)
    
    fail_msg = state.get("fail_msg")
    if fail_msg:
        response = await stream_fixed_message(fail_msg)
        return {"messages": [response], "final_answer": response.content}
    
    fallback_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë§ì”€í•˜ì‹  í–¥ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ì •í™•íˆ íŒŒì•…í•˜ì§€ ëª»í–ˆì–´ìš”. ğŸ˜…\n'ìƒ¤ë„¬ ë„˜ë²„5ë‘ ë¹„ìŠ·í•œ ê±° ì¶”ì²œí•´ì¤˜' ì²˜ëŸ¼ í–¥ìˆ˜ ì´ë¦„ì„ ì½• ì§‘ì–´ì„œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
    response = await stream_fixed_message(fallback_msg)
    return {"messages": [response], "final_answer": response.content}


# ==========================================
# 5. Graph Build (Info Subgraph)
# ==========================================
info_workflow = StateGraph(InfoState)

info_workflow.add_node("info_supervisor", info_supervisor_node)
info_workflow.add_node("perfume_describer", perfume_describer_node)
info_workflow.add_node("ingredient_specialist", ingredient_specialist_node)
info_workflow.add_node("similarity_curator", similarity_curator_node)
info_workflow.add_node("fallback_handler", fallback_handler_node)

info_workflow.add_edge(START, "info_supervisor")

info_workflow.add_conditional_edges(
    "info_supervisor",
    lambda x: x["info_type"],
    {
        "perfume": "perfume_describer",
        "brand": "perfume_describer",
        "note": "ingredient_specialist",
        "accord": "ingredient_specialist",
        "ingredient": "ingredient_specialist",
        "similarity": "similarity_curator",
        "unknown": "fallback_handler",
    },
)

info_workflow.add_edge("perfume_describer", END)
info_workflow.add_edge("ingredient_specialist", END)
info_workflow.add_edge("similarity_curator", END)
info_workflow.add_edge("fallback_handler", END)

info_graph = info_workflow.compile()
