import os
import json
import traceback
from typing import Literal, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
import itertools
import copy

# [Import] ë¡œì»¬ ëª¨ë“ˆ - schemas.pyì—ì„œ ì •ì˜í•œ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision,
    ResearchActionPlan,
    SearchStrategyPlan,
    HardFilters,
    StrategyFilters,
    ResearcherOutput,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
)

from .prompts import (
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_CHAT_PROMPT,
    WRITER_RECOMMENDATION_PROMPT,
    NOTE_SELECTION_PROMPT,
)

load_dotenv()

# ==========================================
# 1. ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)


# ==========================================
# 3. Node Functions
# ==========================================
def supervisor_node(state: AgentState):
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ëŒ€í™” ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ ì¤‘...", flush=True)

    # 1. ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (íœ˜ë°œ ë°©ì§€)
    current_prefs = state.get("user_preferences", {})

    # 2. ì´ë¯¸ ì¸í„°ë·° ëª¨ë“œë¼ë©´ ë°”ë¡œ ì¸í„°ë·°ì–´ë¡œ í† ìŠ¤
    if state.get("active_mode") == "interviewer":
        print("   -> â© Active Mode: Interviewer ìœ ì§€", flush=True)
        return {"next_step": "interviewer"}

    # 3. ì¸í„°ë·°ì–´ì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
    # Supervisor ë‹¨ê³„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì•¼ Researcherê°€ ë¹ˆ ê°’ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.
    messages = [SystemMessage(content=INTERVIEWER_PROMPT)] + state["messages"]

    try:
        # ì •ë°€í•œ ì •ë³´ ì¶”ì¶œì„ ìœ„í•´ SMART_LLM(gpt-4o)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)

        # 4. ì •ë³´ ì—…ë°ì´íŠ¸ ë° ë³‘í•©
        new_prefs = result.user_preferences.dict(exclude_unset=True)
        updated_prefs = {
            **current_prefs,
            **{k: v for k, v in new_prefs.items() if v is not None},
        }

        # 5. ë¼ìš°íŒ… íŒë‹¨
        # [Case A] í–¥ìˆ˜ì™€ ê´€ë ¨ ì—†ëŠ” ì¡ë‹´ì¸ ê²½ìš°
        if result.is_off_topic:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: WRITER (Off-topic)", flush=True)
            return {"next_step": "writer", "active_mode": None}

        # [Case B] í•„ìˆ˜ ì •ë³´(Target + Concept)ê°€ ì¶©ì¡±ëœ ê²½ìš° -> ë°”ë¡œ ê²€ìƒ‰
        if result.is_sufficient:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: RESEARCHER (ì •ë³´ ì¶©ì¡±)", flush=True)
            print(
                f"      ìˆ˜ì§‘ ì •ë³´: {json.dumps(updated_prefs, ensure_ascii=False)}",
                flush=True,
            )
            return {
                "next_step": "researcher",
                "user_preferences": updated_prefs,
                "active_mode": None,
            }

        # [Case C] ì •ë³´ê°€ ë” í•„ìš”í•œ ê²½ìš° -> Interviewerì—ê²Œ ì „ë‹¬
        else:
            print(f"   -> ğŸ¯ ê²°ì •ëœ ê²½ë¡œ: INTERVIEWER (ì¶”ê°€ ì§ˆë¬¸ í•„ìš”)", flush=True)
            return {
                "next_step": "interviewer",
                "user_preferences": updated_prefs,
                "active_mode": "interviewer",
            }

    except Exception as e:
        print(f"   -> âš ï¸ Supervisor Error: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì¸í„°ë·°ì–´ ë‹¨ê³„ë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        return {"next_step": "interviewer"}


# ==========================================
# 4. Interviewer ë…¸ë“œ ì •ì˜
# ==========================================


def interviewer_node(state: AgentState):
    print(f"\nğŸ¤ [Interviewer] ì •ë³´ ë¶„ì„ ì¤‘...", flush=True)
    current_prefs = state.get("user_preferences", {})
    current_prefs_str = (
        json.dumps(current_prefs, ensure_ascii=False, indent=2)
        if current_prefs
        else "ì—†ìŒ"
    )

    augmented_prompt = f"""
    {INTERVIEWER_PROMPT}
    [â˜…Contextâ˜…] ì´ì „ ìˆ˜ì§‘ ì •ë³´: {current_prefs_str}
    """
    messages = [SystemMessage(content=augmented_prompt)] + state["messages"]

    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        print(
            f"   -> ğŸ“Š íŒë‹¨: ì¶©ì¡±({result.is_sufficient}), ì¡ë‹´({result.is_off_topic})",
            flush=True,
        )

        if result.is_off_topic:
            return {"active_mode": None, "next_step": "writer"}

        if result.is_sufficient:
            print("   -> ğŸš€ ì •ë³´ ì¶©ì¡±! Researcher í˜¸ì¶œ", flush=True)
            return {
                "messages": [],
                "user_preferences": result.user_preferences.dict(),
                "active_mode": None,
                "next_step": "researcher",
            }
        else:
            print("   -> â“ ì •ë³´ ë¶€ì¡±", flush=True)
            print("í˜„ì¬ ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì •ë³´ :", result.user_preferences.dict())
            return {
                "messages": [AIMessage(content=result.response_message)],
                "user_preferences": result.user_preferences.dict(),
                "active_mode": "interviewer",
                "next_step": "end",
            }
    except Exception as e:
        print(f"   -> âš ï¸ Error: {e}")
        return {"active_mode": None, "next_step": "writer"}


# ==========================================
# 5. Researcherì— ì‚¬ìš©ë  ê¸°ëŠ¥í•¨ìˆ˜ ì •ì˜
# ==========================================
def log_filters(h_filters: dict, s_filters: dict):
    """í˜„ì¬ ì ìš© ì¤‘ì¸ í•„í„° ì¡°ê±´ì„ ê°€ë…ì„± ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    # Hard Filter í¬ë§·íŒ…
    h_items = [f"{k.capitalize()}: {v}" for k, v in h_filters.items() if v]
    h_str = " | ".join(h_items) if h_items else "None"
    
    # Soft Filter í¬ë§·íŒ…
    s_items = []
    for k, v in s_filters.items():
        if v:
            # ë¦¬ìŠ¤íŠ¸ë©´ ê°„ê²°í•˜ê²Œ í‘œì‹œ
            val_str = str(v) if not isinstance(v, list) else f"{v}"
            s_items.append(f"{k.capitalize()}: {val_str}")
    s_str = " | ".join(s_items) if s_items else "None"

    print(f"       ğŸ”’ [Hard] {h_str}", flush=True)
    print(f"       âœ¨ [Soft] {s_str}", flush=True)

# ==========================================
# [Helper] ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ ë° ì¬ì‹œë„ ë¡œì§ (ì¡°í•©í˜• ì‹œë„)
# ==========================================
# backend/graph.py

def smart_search_with_retry(
    h_filters: dict, 
    s_filters: dict, 
    exclude_ids: list = None,
    query_text: str = "" 
):

    # ì¤‘ìš”ë„ ìˆœì„œ: Note > Accord > Occasion
    priority_order = ["note", "accord", "occasion"]
    active_keys = [k for k in priority_order if k in s_filters and s_filters[k]]

    # ---------------------------------------------------------
    # 1. [Attempt 1] Full Conditions (ë¡œê·¸ ìœ ì§€)
    # ---------------------------------------------------------
    print(f"\n      ğŸ“ [Attempt 1] Full Conditions ({len(active_keys)} filters)", flush=True)
    log_filters(h_filters, s_filters)

    results = advanced_perfume_search_tool.invoke(
        {
            "hard_filters": h_filters,
            "strategy_filters": s_filters,
            "exclude_ids": exclude_ids,
            "query_text": query_text,
        }
    )

    if results:
        print(f"      âœ… Found {len(results)} perfumes (Perfect Match)", flush=True)
        return results, "Perfect Match"

    # ---------------------------------------------------------
    # 2. [Loop] Combinations (ë¡œê·¸ ì œê±° -> ì„±ê³µ ì‹œì—ë§Œ ì¶œë ¥)
    # ---------------------------------------------------------
    for r in range(len(active_keys) - 1, 0, -1):
        # ì¤‘ìš”ë„ ìˆœì„œëŒ€ë¡œ ì¡°í•© ìƒì„±
        combinations = list(itertools.combinations(active_keys, r))
        
        # [ìˆ˜ì •] "Trying..." ë¡œê·¸ ì œê±° (ì¡°ìš©íˆ ì‹œë„)
        
        for combo_keys in combinations:
            temp_filters = {k: s_filters[k] for k in combo_keys}
            combo_str = "+".join([k.upper() for k in combo_keys])
            
            # [ìˆ˜ì •] "Testing..." ë¡œê·¸ ì œê±° (ì¡°ìš©íˆ ì‹œë„)

            results = advanced_perfume_search_tool.invoke(
                {
                    "hard_filters": h_filters,
                    "strategy_filters": temp_filters,
                    "exclude_ids": exclude_ids,
                    "query_text": query_text,
                }
            )

            if results:
                # [ìˆ˜ì •] ì„±ê³µ ì‹œ Levelê³¼ ì¡°í•©ëª…(Combo)ì„ ëª…ì‹œ
                level = len(active_keys) - r
                match_type = f"Relaxed (Level {level} - [{combo_str}])"
                print(f"      âœ… Found {len(results)} perfumes ({match_type})", flush=True)
                return results, match_type

    return [], "No Results"

# ==========================================
# 6. Researcherë…¸ë“œ ì •ì˜
# ==========================================
# backend/graph.py

def researcher_node(state: AgentState):
    print(f"\nğŸ§  [Researcher] ì „ëµ ìˆ˜ë¦½ ë° DB ê²€ìƒ‰...", flush=True)

    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)
    print(f"   ğŸ‘¤ User Context: {current_context}", flush=True)

    # [1] Hard Filterìš© ë…¸íŠ¸ ì „ì²˜ë¦¬
    user_note = user_prefs.get("note")
    refined_hard_note = None
    if user_note:
        matched_notes = lookup_note_by_string_tool.invoke({"keyword": user_note})
        if matched_notes:
            refined_hard_note = matched_notes[0]
            print(
                f"   ğŸ¯ User Note Refined: '{user_note}' -> '{refined_hard_note}'",
                flush=True,
            )

    # [2] ì „ëµ ìˆ˜ë¦½ ë©”ì‹œì§€ ìƒì„±
    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(
            content=f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {current_context}\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì´ë¯¸ì§€ ê°•ì¡°, ë³´ì™„, ë°˜ì „'ì˜ 3ê°€ì§€ ê²€ìƒ‰ ì „ëµì„ ì„¸ì›Œì£¼ì„¸ìš”."
        ),
    ]

    try:
        plan_result = SMART_LLM.with_structured_output(ResearchActionPlan).invoke(
            messages
        )
        final_results = []
        collected_ids = []

        for plan in plan_result.plans:
            print(f"\n   " + "-" * 50, flush=True)
            print(f"   ğŸ‘‰ [Strategy {plan.priority}] {plan.strategy_name}", flush=True)

            current_reason = plan.reason
            
            # ë¦¬ë­í‚¹ìš© ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„± (ê¸°ë³¸ê°’)
            search_query_text = (
                f"{current_reason}. Keywords: {', '.join(plan.strategy_keyword)}"
            )

            h_filters = (
                plan.hard_filters.model_dump(exclude_none=True)
                if hasattr(plan.hard_filters, "model_dump")
                else plan.hard_filters.dict(exclude_none=True)
            )

            # [ì•ˆì „ì¥ì¹˜] ê¸°ë³¸ ë§¤í•‘
            if h_filters.get("season") == "ë´„": h_filters["season"] = "Spring"
            if h_filters.get("gender") == "ë‚¨ì„±": h_filters["gender"] = "Men"
            if refined_hard_note: h_filters["note"] = refined_hard_note

            # =================================================================
            # [â˜…ìˆ˜ì • í¬ì¸íŠ¸] Occasion ì´ì›í™” ì „ëµ (Dual-Track Strategy)
            # =================================================================
            target_occasion = h_filters.get("occasion")
            if target_occasion:
                # DB ë©”íƒ€ë°ì´í„° ë¡œë“œ (ìœ íš¨ì„± ê²€ì‚¬ë¥¼ ìœ„í•´ í•„ìš”)
                from .database import fetch_meta_data
                meta = fetch_meta_data()
                
                # DBì— ìˆëŠ” ìœ íš¨í•œ ìƒí™© ëª©ë¡ (ì†Œë¬¸ì ë³€í™˜í•˜ì—¬ ë¹„êµ)
                valid_occasions = [o.strip().lower() for o in meta.get("occasions", "").split(",")]
                
                if target_occasion.lower() in valid_occasions:
                    # Case A: DBì— ìˆëŠ” ê°’ (ì˜ˆ: Office, Date) -> Hard Filter ìœ ì§€
                    print(f"      ğŸ”’ Occasion '{target_occasion}' is valid. Keeping Hard Filter.", flush=True)
                else:
                    # Case B: DBì— ì—†ëŠ” ê°’ (ì˜ˆ: Wedding, Gym) -> Hard Filter ì œê±° & ì¿¼ë¦¬ì— ì¶”ê°€
                    print(f"      âš ï¸ Occasion '{target_occasion}' not in DB. Moving to Query Text.", flush=True)
                    
                    # 1. SQL ì¡°ê±´ì—ì„œ ì‚­ì œ (0ê±´ ë°©ì§€)
                    del h_filters["occasion"]
                    
                    # 2. ë¦¬ë·° ê²€ìƒ‰ì–´(Query)ì— ì¶”ê°€í•˜ì—¬ ë¦¬ë­í‚¹ìœ¼ë¡œ ì°¾ìŒ
                    search_query_text += f". It is perfect for {target_occasion}."
            # =================================================================

            s_filters = (
                plan.strategy_filters.model_dump(exclude_none=True)
                if hasattr(plan.strategy_filters, "model_dump")
                else plan.strategy_filters.dict(exclude_none=True)
            )

            # [3] Strategy Filterìš© ë…¸íŠ¸ í›„ë³´êµ° ì¶”ì¶œ
            strategy_note_input = s_filters.get("note")
            if strategy_note_input:
                raw_keyword = (
                    strategy_note_input[0]
                    if isinstance(strategy_note_input, list) and strategy_note_input
                    else strategy_note_input
                )

                if raw_keyword:
                    print(
                        f"      ğŸ” '{raw_keyword}' ê¸°ë°˜ ë…¸íŠ¸ í›„ë³´êµ° ì¶”ì¶œ ì¤‘...",
                        flush=True,
                    )
                    candidates = lookup_note_by_vector_tool.invoke(
                        {"keyword": raw_keyword}
                    )

                    if candidates:
                        print(f"      â¡ï¸ ì¶”ì¶œëœ í›„ë³´êµ°: {candidates}", flush=True)
                        selection_messages = [
                            SystemMessage(
                                content=NOTE_SELECTION_PROMPT.format(
                                    candidates=candidates
                                )
                            ),
                            HumanMessage(
                                content=f"í˜„ì¬ ì „ëµ: {plan.strategy_name}\nì˜ë„: {current_reason}"
                            ),
                        ]
                        selected_response = SMART_LLM.invoke(selection_messages).content
                        
                        llm_selected = [
                            c for c in candidates
                            if c.lower() in selected_response.lower()
                        ]

                        s_filters["note"] = (
                            llm_selected if llm_selected else candidates[:1]
                        )
                        print(
                            f"      ğŸ¯ LLM ìµœì¢… ì„ íƒ ë…¸íŠ¸: {s_filters['note']}",
                            flush=True,
                        )

            # [4] ê²€ìƒ‰ ìˆ˜í–‰ (ë¦¬ë­í‚¹ ì ìš©)
            db_perfumes, match_type = smart_search_with_retry(
                h_filters,
                s_filters,
                exclude_ids=collected_ids,
                query_text=search_query_text,
            )

            # [5] ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ Re-Act
            if not db_perfumes:
                print(
                    f"      âš ï¸ '{plan.strategy_name}' ê²°ê³¼ ì—†ìŒ. ì¬ìˆ˜ë¦½ ì‹œë„...",
                    flush=True,
                )
                retry_messages = [
                    SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
                    HumanMessage(
                        content=f"ì‚¬ìš©ì ì •ë³´: {current_context}\nì‹¤íŒ¨í•œ í•„í„°: {json.dumps(s_filters)}\nì „ëµì— ë¶€í•©í•˜ëŠ” ìƒˆë¡œìš´ í‚¤ì›Œë“œì™€ ì‚¬ìœ (Reason)ë¥¼ ì œì•ˆí•´ì¤˜."
                    ),
                ]
                new_plan = SMART_LLM.with_structured_output(SearchStrategyPlan).invoke(
                    retry_messages
                )

                s_filters = (
                    new_plan.strategy_filters.model_dump(exclude_none=True)
                    if hasattr(new_plan.strategy_filters, "model_dump")
                    else new_plan.strategy_filters.dict(exclude_none=True)
                )
                current_reason = new_plan.reason
                
                # ì¬ì‹œë„ ì‹œì—ë„ ì¿¼ë¦¬ ì—…ë°ì´íŠ¸
                search_query_text = f"{current_reason}. Keywords: {', '.join(new_plan.strategy_keyword)}"
                
                # ì¬ì‹œë„ ì‹œì—ë„ Occasionì´ ì¿¼ë¦¬ì— ë°˜ì˜ë˜ì–´ì•¼ í•œë‹¤ë©´ ì¶”ê°€ (ì„ íƒì‚¬í•­)
                if target_occasion and target_occasion.lower() not in valid_occasions:
                     search_query_text += f". It is perfect for {target_occasion}."

                # ë…¸íŠ¸ ì¬ê²€ìƒ‰ ë¡œì§
                if s_filters.get("note"):
                    retry_keyword = (
                        s_filters["note"][0]
                        if isinstance(s_filters["note"], list)
                        else s_filters["note"]
                    )
                    retry_candidates = lookup_note_by_vector_tool.invoke(
                        {"keyword": retry_keyword}
                    )
                    if retry_candidates:
                        s_filters["note"] = retry_candidates[:2]

                db_perfumes, match_type = smart_search_with_retry(
                    h_filters,
                    s_filters,
                    exclude_ids=collected_ids,
                    query_text=search_query_text,
                )

            # [6] ê²°ê³¼ ì •ë¦¬
            perfume_details = []
            if db_perfumes:
                p = db_perfumes[0]
                collected_ids.append(p["id"])
                print(
                    f"      âœ… ìµœì¢… ì„ ì •: {p.get('brand')} - {p.get('name')} ({match_type})",
                    flush=True,
                )

                best_review_text = p.get("best_review", "ë¦¬ë·° ì •ë³´ ì—†ìŒ")
                accord_with_review = f"{p.get('accords') or 'ì •ë³´ ì—†ìŒ'}\n[âœ¨ Best Review]: {best_review_text}"

                p_notes = PerfumeNotes(
                    top=p.get("top_notes") or "ì •ë³´ ì—†ìŒ",
                    middle=p.get("middle_notes") or "ì •ë³´ ì—†ìŒ",
                    base=p.get("base_notes") or "ì •ë³´ ì—†ìŒ",
                )
                detail = PerfumeDetail(
                    perfume_name=p.get("name", "Unknown"),
                    perfume_brand=p.get("brand", "Unknown"),
                    accord=accord_with_review,
                    season="All Seasons",
                    occasion="Any",
                    gender=p.get("gender", "Unisex"),
                    notes=p_notes,
                    image_url=p.get("image_url"),
                )
                perfume_details.append(detail)

            final_results.append(
                StrategyResult(
                    strategy_name=plan.strategy_name,
                    strategy_keyword=plan.strategy_keyword,
                    strategy_reason=current_reason,
                    perfumes=perfume_details,
                )
            )

        return {
            "research_results": (
                ResearcherOutput(results=final_results).model_dump()
                if hasattr(ResearcherOutput, "model_dump")
                else ResearcherOutput(results=final_results).dict()
            ),
            "messages": [AIMessage(content="[RESEARCH_DONE]")],
            "next_step": "writer",
        }

    except Exception as e:
        print(f"   -> ğŸš¨ Researcher Node Error: {e}")
        import traceback

        traceback.print_exc()
        return {"research_results": {"results": []}, "next_step": "writer"}


# ==========================================
# 7. Writerë…¸ë“œ ì •ì˜ (ë¹„ë™ê¸° ì²˜ë¦¬ ì ìš©)
# ==========================================


async def writer_node(state: AgentState):
    print(f"\nâœï¸ [Writer] ìµœì¢… ë‹µë³€ ì‘ì„± ì¤‘...", flush=True)
    last_message = state["messages"][-1]
    research_data = state.get("research_results", {})
    results_list = research_data.get("results", [])

    if isinstance(last_message, HumanMessage):
        selected_prompt = WRITER_CHAT_PROMPT
        data_context = ""
    elif not results_list or all(len(r["perfumes"]) == 0 for r in results_list):
        selected_prompt = WRITER_FAILURE_PROMPT
        data_context = ""
    else:
        selected_prompt = WRITER_RECOMMENDATION_PROMPT
        data_context = json.dumps(research_data, ensure_ascii=False, indent=2)

    full_content = f"{selected_prompt}\n\n[ì°¸ê³  ë°ì´í„°]:\n{data_context}"
    messages = [SystemMessage(content=full_content)] + state["messages"]

    try:
        # ainvokeë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤.
        # astream_eventsê°€ ì´ ë‚´ë¶€ì˜ ìŠ¤íŠ¸ë¦¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
        response = await SUPER_SMART_LLM.ainvoke(messages)
        return {"messages": [response], "next_step": "end"}
    except Exception:
        return {"next_step": "end"}


# 4. Graph Build
workflow = StateGraph(AgentState)
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("writer", writer_node)

workflow.add_edge(START, "supervisor")
workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {"interviewer": "interviewer", "researcher": "researcher", "writer": "writer"},
)
workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {"end": END, "researcher": "researcher", "writer": "writer"},
)
workflow.add_edge("researcher", "writer")
workflow.add_edge("writer", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
