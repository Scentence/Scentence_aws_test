import os
import json
import traceback
import asyncio
import itertools
from typing import Literal, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# [Import] ë¡œì»¬ ëª¨ë“ˆ - schemas.pyì˜ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision,
    ResearchActionPlan,
    SearchStrategyPlan,
    HardFilters,
    StrategyFilters,
    ResearcherOutput,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
)

from .prompts import (
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_CHAT_PROMPT,
    WRITER_RECOMMENDATION_PROMPT,
    NOTE_SELECTION_PROMPT,
)
from .database import save_recommendation_log

load_dotenv()

# ==========================================
# 1. ëª¨ë¸ ì„¤ì • (ì„±ëŠ¥ ì´ì›í™”)
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4.1", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)

# ==========================================
# 2. ìœ í‹¸ë¦¬í‹° ë° ë³´ì¡° ê¸°ëŠ¥ í•¨ìˆ˜
# ==========================================


def log_filters(h_filters: dict, s_filters: dict):
    """í˜„ì¬ ì ìš© ì¤‘ì¸ í•„í„° ì¡°ê±´ì„ ê°€ë…ì„± ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    h_items = [f"{k.capitalize()}: {v}" for k, v in h_filters.items() if v]
    h_str = " | ".join(h_items) if h_items else "None"

    s_items = []
    for k, v in s_filters.items():
        if v:
            val_str = str(v) if not isinstance(v, list) else f"{v}"
            s_items.append(f"{k.capitalize()}: {val_str}")
    s_str = " | ".join(s_items) if s_items else "None"

    print(f"       ğŸ”’ [Hard] {h_str}", flush=True)
    print(f"       âœ¨ [Soft] {s_str}", flush=True)


async def smart_search_with_retry_async(
    h_filters: dict, s_filters: dict, exclude_ids: list = None, query_text: str = ""
):
    """í•„í„°ë¥¼ ë‹¨ê³„ë³„ë¡œ ì™„í™”í•˜ë©° ë¹„ë™ê¸°ì ìœ¼ë¡œ í–¥ìˆ˜ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    priority_order = ["note", "accord", "occasion"]
    active_keys = [k for k in priority_order if k in s_filters and s_filters[k]]

    # 1ì°¨ ì‹œë„ (ì „ì²´ ì¡°ê±´)
    results = await advanced_perfume_search_tool.ainvoke(
        {
            "hard_filters": h_filters,
            "strategy_filters": s_filters,
            "exclude_ids": exclude_ids,
            "query_text": query_text,
        }
    )
    if results:
        return results, "Perfect Match"

    # 2ì°¨ ì‹œë„ (í•„í„° ì¡°í•© ì™„í™” ë£¨í”„)
    for r in range(len(active_keys) - 1, 0, -1):
        for combo_keys in itertools.combinations(active_keys, r):
            temp_filters = {k: s_filters[k] for k in combo_keys}
            results = await advanced_perfume_search_tool.ainvoke(
                {
                    "hard_filters": h_filters,
                    "strategy_filters": temp_filters,
                    "exclude_ids": exclude_ids,
                    "query_text": query_text,
                }
            )
            if results:
                return results, f"Relaxed (Level {len(active_keys)-r})"
    return [], "No Results"


# ==========================================
# 3. Node Functions
# ==========================================


def supervisor_node(state: AgentState):
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ëŒ€í™” ë¶„ì„ ë° ì •ë³´ ì¶”ì¶œ ì¤‘...", flush=True)
    current_prefs = state.get("user_preferences", {})

    if state.get("active_mode") == "interviewer":
        return {"next_step": "interviewer"}

    messages = [SystemMessage(content=INTERVIEWER_PROMPT)] + state["messages"]
    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        new_prefs = result.user_preferences.dict(exclude_unset=True)
        updated_prefs = {
            **current_prefs,
            **{k: v for k, v in new_prefs.items() if v is not None},
        }

        if result.is_off_topic:
            return {"next_step": "writer", "active_mode": None}
        if result.is_sufficient:
            return {
                "next_step": "researcher",
                "user_preferences": updated_prefs,
                "status": "ì¶”ì²œ ì „ëµì„ ì„¸ìš°ëŠ” ì¤‘ì…ë‹ˆë‹¤...",
                "active_mode": None,
            }
        return {
            "next_step": "interviewer",
            "user_preferences": updated_prefs,
            "active_mode": "interviewer",
        }
    except Exception:
        return {"next_step": "interviewer"}


def interviewer_node(state: AgentState):
    print(f"\nğŸ¤ [Interviewer] ì •ë³´ ë¶„ì„ ë° ì¶”ê°€ ì§ˆë¬¸ ìƒì„±...", flush=True)
    messages = [SystemMessage(content=INTERVIEWER_PROMPT)] + state["messages"]
    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        if result.is_sufficient:
            return {
                "next_step": "researcher",
                "status": "ì¶”ì²œ ì „ëµì„ ì„¸ìš°ëŠ” ì¤‘ì…ë‹ˆë‹¤...",
                "active_mode": None,
            }
        return {
            "messages": [AIMessage(content=result.response_message)],
            "user_preferences": result.user_preferences.dict(),
            "active_mode": "interviewer",
            "next_step": "end",
        }
    except Exception:
        return {"next_step": "writer"}


async def researcher_node(state: AgentState):
    print(f"\nğŸ§  [Researcher] ì „ëµ ìˆ˜ë¦½ ë° ë³‘ë ¬ DB ê²€ìƒ‰ ì‹œì‘...", flush=True)
    current_member_id = state.get("member_id", 0)
    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)

    # 1. í•˜ë“œ í•„í„°ìš© ë…¸íŠ¸ ì „ì²˜ë¦¬ (ì‚¬ìš©ì ì…ë ¥ ë…¸ë“œë¥¼ DB ê·œê²©ìœ¼ë¡œ ë³€í™˜)
    user_note = user_prefs.get("note")
    refined_hard_note = None
    if user_note:
        matched = await lookup_note_by_string_tool.ainvoke({"keyword": user_note})
        if matched:
            refined_hard_note = matched[0]

    # 2. ì „ëµ ìˆ˜ë¦½ (gpt-4o-mini ì‚¬ìš©ìœ¼ë¡œ ì§€ì—° ì‹œê°„ ë‹¨ì¶•)
    messages = [
        SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
        HumanMessage(
            content=f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {current_context}\nìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì´ë¯¸ì§€ ê°•ì¡°, ë³´ì™„, ë°˜ì „'ì˜ 3ê°€ì§€ ê²€ìƒ‰ ì „ëµì„ ì„¸ì›Œì£¼ì„¸ìš”."
        ),
    ]
    plan_result = await SMART_LLM.with_structured_output(ResearchActionPlan).ainvoke(
        messages
    )

    # 3. ê°œë³„ ì „ëµ ì²˜ë¦¬ ë¹„ë™ê¸° ë‚´ë¶€ í•¨ìˆ˜ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ í›„ë³´êµ° ì „ì²´ ë°˜í™˜)
    async def process_strategy_candidates(plan: SearchStrategyPlan):
        # [ë¡œê·¸] ê° ì „ëµì˜ ì‹œì‘ ì•Œë¦¼
        print(f"   ğŸ‘‰ [Parallel Task Start] {plan.strategy_name}", flush=True)

        h_filters = plan.hard_filters.model_dump(exclude_none=True)
        if refined_hard_note:
            h_filters["note"] = refined_hard_note

        s_filters = plan.strategy_filters.model_dump(exclude_none=True)

        # ì†Œí”„íŠ¸ í•„í„°ìš© ë…¸íŠ¸ ë²¡í„° ê²€ìƒ‰ ë° LLM ìµœì¢… ì„ íƒ
        strategy_note_input = s_filters.get("note")
        if strategy_note_input:
            raw_keyword = (
                strategy_note_input[0]
                if isinstance(strategy_note_input, list)
                else strategy_note_input
            )
            candidates = await lookup_note_by_vector_tool.ainvoke(
                {"keyword": raw_keyword}
            )

            if candidates:
                selection_messages = [
                    SystemMessage(
                        content=NOTE_SELECTION_PROMPT.format(candidates=candidates)
                    ),
                    HumanMessage(
                        content=f"ì „ëµ: {plan.strategy_name}\nì˜ë„: {plan.reason}"
                    ),
                ]
                selected_res = await SMART_LLM.ainvoke(selection_messages)
                llm_selected = [
                    c for c in candidates if c.lower() in selected_res.content.lower()
                ]
                s_filters["note"] = (
                    llm_sel if (llm_sel := llm_selected) else candidates[:1]
                )

        # [ë¡œê·¸] í˜„ì¬ ì ìš©ëœ ìƒì„¸ í•„í„° ì¶œë ¥
        log_filters(h_filters, s_filters)

        # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì „ì²´(candidates)ë¥¼ ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        db_perfumes, match_type = await smart_search_with_retry_async(
            h_filters, s_filters, query_text=plan.reason
        )

        # [ë¡œê·¸] ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(
            f"      âœ… {plan.strategy_name}: {len(db_perfumes)}ê±´ ë°œê²¬ ({match_type})",
            flush=True,
        )

        return {"plan": plan, "candidates": db_perfumes}

    # 4. asyncio.gatherë¥¼ í†µí•´ 3ê°€ì§€ ì „ëµì„ ë³‘ë ¬ë¡œ ë™ì‹œ ìˆ˜í–‰
    tasks = [process_strategy_candidates(p) for p in plan_result.plans]
    all_candidates_results = await asyncio.gather(*tasks)

    # 5. ì¤‘ë³µ ì œê±° ë° ì „ëµë³„ ê³ ìœ  í–¥ìˆ˜ ìµœì¢… ì„ íƒ
    final_results = []
    seen_perfume_ids = set()

    for item in all_candidates_results:
        plan = item["plan"]
        candidates = item["candidates"]

        # ì´ë¯¸ ë‹¤ë¥¸ ì „ëµì—ì„œ ì„ íƒëœ í–¥ìˆ˜ëŠ” ì œì™¸í•˜ê³  ê°€ì¥ ìˆœìœ„ê°€ ë†’ì€ ê²ƒì„ ì„ íƒí•©ë‹ˆë‹¤.
        selected_p = None
        for p in candidates:
            if p["id"] not in seen_perfume_ids:
                selected_p = p
                seen_perfume_ids.add(p["id"])
                break

        # ë§Œì•½ ëª¨ë“  í›„ë³´ê°€ ì¤‘ë³µì´ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ í•´ë‹¹ ì „ëµì€ ê±´ë„ˆëœë‹ˆë‹¤.
        if not selected_p:
            continue

        # DB ë¡œê·¸ ì €ì¥ ë° ê²°ê³¼ ê°ì²´ ìƒì„±
        save_recommendation_log(
            member_id=current_member_id, perfumes=[selected_p], reason=plan.reason
        )

        final_results.append(
            StrategyResult(
                strategy_name=plan.strategy_name,
                strategy_keyword=plan.strategy_keyword,
                strategy_reason=plan.reason,
                perfumes=[
                    PerfumeDetail(
                        id=selected_p.get("id"),
                        perfume_name=selected_p.get("name"),
                        perfume_brand=selected_p.get("brand"),
                        accord=f"{selected_p.get('accords')}\n[Best Review]: {selected_p.get('best_review')}",
                        notes=PerfumeNotes(
                            top=selected_p.get("top_notes") or "N/A",
                            middle=selected_p.get("middle_notes") or "N/A",
                            base=selected_p.get("base_notes") or "N/A",
                        ),
                        image_url=selected_p.get("image_url"),
                        gender=selected_p.get("gender", "Unisex"),
                        season="All",
                        occasion="Any",
                    )
                ],
            )
        )

    # 6. ìµœì¢… ê²°ê³¼ë¥¼ ìƒíƒœì— ë°˜ì˜í•˜ê³  ì‘ê°€ ë…¸ë“œë¡œ ì´ë™
    return {
        "research_results": {"results": [r.dict() for r in final_results]},
        "messages": [AIMessage(content="[RESEARCH_DONE]")],
        "next_step": "writer",
    }


async def writer_node(state: AgentState):
    print(f"\nâœï¸ [Writer] ë‹µë³€ ì‘ì„± ì¤‘...", flush=True)
    research_data = state.get("research_results", {})
    results_list = research_data.get("results", [])

    prompt = WRITER_RECOMMENDATION_PROMPT if results_list else WRITER_FAILURE_PROMPT
    data_ctx = (
        json.dumps(research_data, ensure_ascii=False, indent=2) if results_list else ""
    )

    messages = [
        SystemMessage(content=f"{prompt}\n\n[ì°¸ê³  ë°ì´í„°]:\n{data_ctx}")
    ] + state["messages"]

    response = await SUPER_SMART_LLM.ainvoke(messages)
    return {"messages": [response], "next_step": "end"}


# ==========================================
# 4. Graph Build
# ==========================================
workflow = StateGraph(AgentState)
workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
workflow.add_node("researcher", researcher_node)
workflow.add_node("writer", writer_node)

workflow.add_edge(START, "supervisor")
workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {"interviewer": "interviewer", "researcher": "researcher", "writer": "writer"},
)
workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {"end": END, "researcher": "researcher", "writer": "writer"},
)
workflow.add_edge("researcher", "writer")
workflow.add_edge("writer", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
