# backend/agent/graph_info.py
import json
import asyncio
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END

# [1] ìŠ¤í‚¤ë§ˆ ì„í¬íŠ¸
from .schemas_info import InfoState, InfoRoutingDecision
from .tools_schemas_info import IngredientAnalysisResult

# [2] ë„êµ¬ ì„í¬íŠ¸
from .tools_info import (
    lookup_perfume_info_tool, 
    lookup_note_info_tool, 
    lookup_accord_info_tool
)
from .tools_similarity import lookup_similar_perfumes_tool  

# [3] í”„ë¡¬í”„íŠ¸ ì„í¬íŠ¸
from .prompts_info import (
    INFO_SUPERVISOR_PROMPT,
    PERFUME_DESCRIBER_PROMPT,
    INGREDIENT_SPECIALIST_PROMPT,
    SIMILARITY_CURATOR_PROMPT
)

load_dotenv()

# [LLM ì´ì›í™”]
INFO_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
ROUTER_LLM = ChatOpenAI(model="gpt-4o", temperature=0, streaming=False)


# ==========================================
# 4. Node Functions
# ==========================================

def info_supervisor_node(state: InfoState):
    """[Router] ë¶„ë¥˜ ë…¸ë“œ"""
    print(f"\n   â–¶ï¸ [Info Subgraph] Supervisor ë…¸ë“œ ì‹œì‘", flush=True)
    user_query = state.get('user_query', '')
    
    chat_history = state.get("messages", [])
    context_str = ""
    if chat_history:
        recent_msgs = chat_history[-3:] if len(chat_history) > 3 else chat_history
        for msg in recent_msgs:
            role = "User" if isinstance(msg, HumanMessage) else "AI"
            if msg.content:
                context_str += f"- {role}: {msg.content}\n"

    final_system_prompt = INFO_SUPERVISOR_PROMPT
    if context_str:
        final_system_prompt += f"\n\n[Recent Chat Context]\n{context_str}"
    
    final_system_prompt += "\n\n[Instruction]\nResolve the target name from context and classify based on the PRIORITY rules."

    messages = [
        SystemMessage(content=final_system_prompt),
        HumanMessage(content=user_query)
    ]
    
    try:
        decision = ROUTER_LLM.with_structured_output(InfoRoutingDecision).invoke(messages)
        final_target = decision.target_name
        
        if not final_target or final_target in ["ì´ê±°", "ê·¸ê±°", "ì´ í–¥ìˆ˜", "ì¶”ì²œí•´ì¤˜", "ë¹„ìŠ·í•œê±°"]:
             print(f"      âš ï¸ íƒ€ê²Ÿ í•´ìƒ ì‹¤íŒ¨: '{final_target}' -> Fallback", flush=True)
             return {"info_type": "unknown", "target_name": "unknown"}

        print(f"      ğŸ‘‰ [Decided] Type: '{decision.info_type}' | Target: '{final_target}'", flush=True)
        
        return {
            "info_type": decision.info_type,
            "target_name": final_target
        }
        
    except Exception as e:
        print(f"      âŒ Supervisor ì—ëŸ¬ ë°œìƒ: {e}", flush=True)
        return {"info_type": "unknown", "target_name": "unknown"}


async def perfume_describer_node(state: InfoState):
    """[Perfume Expert] ìƒì„¸ ì •ë³´"""
    try:
        target = state["target_name"]
        print(f"\n   â–¶ï¸ [Info Subgraph] Perfume Describer: '{target}'", flush=True)
        
        search_result_json = await lookup_perfume_info_tool.ainvoke(target)
        print(f"      ğŸ” [DB Result]: {str(search_result_json)[:200]}...", flush=True)
        
        messages = [
            SystemMessage(content=PERFUME_DESCRIBER_PROMPT),
            HumanMessage(content=f"ëŒ€ìƒ í–¥ìˆ˜: {target}\n\n[ê²€ìƒ‰ëœ ìƒì„¸ ì •ë³´]:\n{search_result_json}")
        ]
        response = await INFO_LLM.ainvoke(messages)
        
        # [â˜…ìˆ˜ì •] final_answerì— response.contentë¥¼ ë‹´ì•„ì„œ ë°˜í™˜í•´ì•¼ í™”ë©´ì— ë‚˜ì˜µë‹ˆë‹¤!
        return {
            "messages": [response], 
            "final_answer": response.content
        }
        
    except Exception as e:
        print(f"      âŒ Perfume Describer ì—ëŸ¬: {e}", flush=True)
        msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}' ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return {"messages": [AIMessage(content=msg)], "final_answer": msg}


async def ingredient_specialist_node(state: InfoState):
    """[Ingredient Expert] ì„±ë¶„ ë¶„ì„"""
    try:
        user_query = state.get("user_query", "")
        target_name = state.get("target_name", "") 
        print(f"\n   â–¶ï¸ [Info Subgraph] Ingredient Specialist: '{user_query}'", flush=True)

        analysis_prompt = f"""
        You are a query analyzer. Separate 'Notes' and 'Accords'.
        Query: "{user_query}"
        Context Target: "{target_name}"
        Output JSON: {{ "notes": [], "accords": [], "is_ambiguous": false }}
        """
        
        try:
            analysis = await ROUTER_LLM.with_structured_output(IngredientAnalysisResult).ainvoke(
                analysis_prompt,
                config={"tags": ["internal_helper"]} 
            )
            print(f"      - ë¶„ì„ ê²°ê³¼: Notes={analysis.notes}, Accords={analysis.accords}", flush=True)
        except Exception as e:
            print(f"      âš ï¸ ë¶„ì„ ì‹¤íŒ¨: {e}", flush=True)
            analysis = IngredientAnalysisResult(notes=[target_name], accords=[])

        tasks = []
        tasks.append(lookup_note_info_tool.ainvoke({"keywords": analysis.notes}) if analysis.notes else asyncio.sleep(0, result=""))
        tasks.append(lookup_accord_info_tool.ainvoke({"keywords": analysis.accords}) if analysis.accords else asyncio.sleep(0, result=""))
        
        results = await asyncio.gather(*tasks)
        note_result, accord_result = results[0], results[1]

        def print_result_log(category: str, result_str: str):
            if not result_str: return
            try:
                data = json.loads(result_str)
                if not data:
                    print(f"      ğŸ” [{category}]: ê²°ê³¼ ì—†ìŒ (Empty)", flush=True)
                    return
                for key, val in data.items():
                    if isinstance(val, dict):
                        perfumes = val.get("representative_perfumes", [])
                        perfume_log = ", ".join(perfumes) if perfumes else "ì—†ìŒ"
                        print(f"      ğŸ” [{category}] '{key}': (ëŒ€í‘œí–¥ìˆ˜: {perfume_log})", flush=True)
            except: pass

        print_result_log("Note DB", note_result)
        print_result_log("Accord DB", accord_result)
        
        combined_context = f"""
        [User Interest]: Notes: {analysis.notes}, Accords: {analysis.accords}
        [Search Results]:
        --- Note Data ---
        {note_result}
        --- Accord Data ---
        {accord_result}
        """
        
        messages = [
            SystemMessage(content=INGREDIENT_SPECIALIST_PROMPT),
            HumanMessage(content=combined_context)
        ]
        response = await INFO_LLM.ainvoke(messages)
        
        # [â˜…ìˆ˜ì •] final_answer ì¶”ê°€
        return {
            "messages": [response], 
            "final_answer": response.content
        }
        
    except Exception as e:
        print(f"      âŒ Ingredient Specialist ì—ëŸ¬: {e}", flush=True)
        msg = "ì„±ë¶„ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ë„ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        return {"messages": [AIMessage(content=msg)], "final_answer": msg}


async def similarity_curator_node(state: InfoState):
    """[Similarity Curator] ìœ ì‚¬ í–¥ìˆ˜ ì¶”ì²œ"""
    try:
        target = state["target_name"]
        print(f"\n   â–¶ï¸ [Info Subgraph] Similarity Curator: '{target}'", flush=True)
        
        # 1. ë„êµ¬ ì‹¤í–‰
        similarity_result_json = await lookup_similar_perfumes_tool.ainvoke(target)
        print(f"      ğŸ” [Similarity Result]: {str(similarity_result_json)[:200]}...", flush=True)
        
        # 2. ë‹µë³€ ìƒì„±
        messages = [
            SystemMessage(content=SIMILARITY_CURATOR_PROMPT),
            HumanMessage(content=f"ê¸°ì¤€ í–¥ìˆ˜: {target}\n\n[ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼]:\n{similarity_result_json}")
        ]
        response = await INFO_LLM.ainvoke(messages)
        
        # [â˜…ìˆ˜ì •] final_answer ì¶”ê°€
        return {
            "messages": [response], 
            "final_answer": response.content
        }
        
    except Exception as e:
        print(f"      âŒ Similarity Curator ì—ëŸ¬: {e}", flush=True)
        msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{target}'ì™€ ìœ ì‚¬í•œ í–¥ìˆ˜ë¥¼ ì°¾ëŠ” ê³¼ì •ì—ì„œ ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ì–´ì ¸ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        return {"messages": [AIMessage(content=msg)], "final_answer": msg}


async def fallback_handler_node(state: InfoState):
    """[Fallback] ì•ˆë‚´"""
    print(f"\n   âš ï¸ [Info Subgraph] Fallback Handler ì‹¤í–‰", flush=True)
    fallback_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ë§ì”€í•˜ì‹  í–¥ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ ì •í™•íˆ íŒŒì•…í•˜ì§€ ëª»í–ˆì–´ìš”. ğŸ˜…\n'ìƒ¤ë„¬ ë„˜ë²„5ë‘ ë¹„ìŠ·í•œ ê±° ì¶”ì²œí•´ì¤˜' ì²˜ëŸ¼ í–¥ìˆ˜ ì´ë¦„ì„ ì½• ì§‘ì–´ì„œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
    
    # [â˜…ìˆ˜ì •] final_answer ì¶”ê°€
    return {
        "messages": [AIMessage(content=fallback_msg)], 
        "final_answer": fallback_msg
    }


# ==========================================
# 5. Graph Build (Info Subgraph)
# ==========================================
info_workflow = StateGraph(InfoState)

info_workflow.add_node("info_supervisor", info_supervisor_node)
info_workflow.add_node("perfume_describer", perfume_describer_node)
info_workflow.add_node("ingredient_specialist", ingredient_specialist_node) 
info_workflow.add_node("similarity_curator", similarity_curator_node) 
info_workflow.add_node("fallback_handler", fallback_handler_node)

info_workflow.add_edge(START, "info_supervisor")

info_workflow.add_conditional_edges(
    "info_supervisor",
    lambda x: x["info_type"],
    {
        "perfume": "perfume_describer",
        "brand": "perfume_describer",      
        "note": "ingredient_specialist",   
        "accord": "ingredient_specialist", 
        "ingredient": "ingredient_specialist",
        "similarity": "similarity_curator",
        "unknown": "fallback_handler"
    }
)

info_workflow.add_edge("perfume_describer", END)
info_workflow.add_edge("ingredient_specialist", END)
info_workflow.add_edge("similarity_curator", END)
info_workflow.add_edge("fallback_handler", END)

info_graph = info_workflow.compile()