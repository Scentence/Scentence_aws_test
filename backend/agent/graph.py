# backend/agent/graph.py
import os
import json
import asyncio
import itertools
from typing import Literal, List, Dict, Any, Optional

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# [Import] ë¡œì»¬ ëª¨ë“ˆ
from .schemas import (
    AgentState,
    UserPreferences,
    InterviewResult,
    RoutingDecision,
    ResearchActionPlan,
    SearchStrategyPlan,
    ResearcherOutput,
    StrategyResult,
    PerfumeDetail,
    PerfumeNotes,
)

from .tools import (
    advanced_perfume_search_tool,
    lookup_note_by_string_tool,
    lookup_note_by_vector_tool,
)

from .prompts import (
    SUPERVISOR_PROMPT,
    INTERVIEWER_PROMPT,
    RESEARCHER_SYSTEM_PROMPT,
    WRITER_FAILURE_PROMPT,
    WRITER_CHAT_PROMPT,
    WRITER_RECOMMENDATION_PROMPT,
    WRITER_RECOMMENDATION_PROMPT_EXPERT,
    NOTE_SELECTION_PROMPT,
)
from .database import save_recommendation_log

# [ì •ë³´ ê²€ìƒ‰ ì „ìš© ì„œë¸Œ ê·¸ë˜í”„ ì„í¬íŠ¸]
from .graph_info import info_graph

load_dotenv()

# ==========================================
# 1. ëª¨ë¸ ì„¤ì •
# ==========================================
FAST_LLM = ChatOpenAI(model="gpt-4.1-mini", temperature=0, streaming=True)
SMART_LLM = ChatOpenAI(model="gpt-4.1", temperature=0, streaming=True)
SUPER_SMART_LLM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=True)
# Non-streaming version for parallel_reco to prevent token interleaving
SUPER_SMART_LLM_NO_STREAM = ChatOpenAI(model="gpt-5.2", temperature=0, streaming=False)


# ==========================================
# 2. ìœ í‹¸ë¦¬í‹°
# ==========================================
def log_filters(h_filters: dict, s_filters: dict):
    h_items = [f"{k.capitalize()}: {v}" for k, v in h_filters.items() if v]
    h_str = " | ".join(h_items) if h_items else "None"

    s_items = []
    for k, v in s_filters.items():
        if v:
            val_str = str(v) if not isinstance(v, list) else f"{v}"
            s_items.append(f"{k.capitalize()}: {val_str}")
    s_str = " | ".join(s_items) if s_items else "None"

    print(f"       ğŸ”’ [Hard] {h_str}", flush=True)
    print(f"       âœ¨ [Soft] {s_str}", flush=True)


async def smart_search_with_retry_async(
    h_filters: dict, s_filters: dict, exclude_ids: list = None, query_text: str = ""
):
    priority_order = ["note", "accord", "occasion"]
    active_keys = [k for k in priority_order if k in s_filters and s_filters[k]]

    results = await advanced_perfume_search_tool.ainvoke(
        {
            "hard_filters": h_filters,
            "strategy_filters": s_filters,
            "exclude_ids": exclude_ids,
            "query_text": query_text,
        }
    )
    if results:
        return results, "Perfect Match"

    for r in range(len(active_keys) - 1, 0, -1):
        for combo_keys in itertools.combinations(active_keys, r):
            temp_filters = {k: s_filters[k] for k in combo_keys}
            results = await advanced_perfume_search_tool.ainvoke(
                {
                    "hard_filters": h_filters,
                    "strategy_filters": temp_filters,
                    "exclude_ids": exclude_ids,
                    "query_text": query_text,
                }
            )
            if results:
                return results, f"Relaxed (Level {len(active_keys)-r})"
    return [], "No Results"


async def call_info_graph_wrapper(state: AgentState):
    """Sub-Graph Wrapper"""
    print(f"\nğŸš€ [Main Graph] 'info_graph' ì„œë¸Œ ê·¸ë˜í”„ í˜¸ì¶œ...", flush=True)
    current_query = state.get("user_query", "")

    if not current_query and state.get("messages"):
        last_msg = state["messages"][-1]
        if isinstance(last_msg, HumanMessage):
            current_query = last_msg.content

    print(f"   ğŸ‘‰ ì „ë‹¬í•  Query: {current_query}", flush=True)

    subgraph_input = {
        "user_query": current_query,
        "messages": state.get("messages", []),
    }

    try:
        result = await info_graph.ainvoke(subgraph_input)
        print(f"âœ… [Main Graph] ì„œë¸Œ ê·¸ë˜í”„ ì™„ë£Œ. ê²°ê³¼ ë³µê·€.", flush=True)
        return {"messages": result.get("messages", [])}

    except Exception as e:
        print(f"ğŸš¨ [Main Graph] ì„œë¸Œ ê·¸ë˜í”„ ì—ëŸ¬: {e}", flush=True)
        import traceback

        traceback.print_exc()
        return {"messages": [AIMessage(content="ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")]}


# ==========================================
# 3. Node Functions
# ==========================================


def supervisor_node(state: AgentState):
    """[Main Router]"""
    print("\n" + "=" * 60, flush=True)
    print("ğŸ‘€ [Supervisor] ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜ ì¤‘...", flush=True)

    if state.get("active_mode") == "interviewer":
        print("   ğŸ‘‰ ì¸í„°ë·° ì§„í–‰ ì¤‘ -> Interviewerë¡œ ì´ë™", flush=True)
        return {"next_step": "interviewer"}

    messages = [SystemMessage(content=SUPERVISOR_PROMPT)] + state["messages"]

    try:
        decision = SMART_LLM.with_structured_output(RoutingDecision).invoke(messages)
        next_step = decision.next_step
        print(f"   ğŸ‘‰ ë¶„ë¥˜ ê²°ê³¼: {next_step}", flush=True)
        return {"next_step": next_step}

    except Exception as e:
        print(f"   âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨(Error): {e} -> ê¸°ë³¸ê°’ Writerë¡œ ì´ë™", flush=True)
        return {"next_step": "writer"}


def interviewer_node(state: AgentState):
    """[Interviewer]"""
    print(f"\nğŸ¤ [Interviewer] ì¶”ì²œ ì •ë³´ ë¶„ì„ ë° ê²€ì¦...", flush=True)

    current_prefs = state.get("user_preferences", {})

    # í˜„ì¬ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    current_context_str = json.dumps(current_prefs, ensure_ascii=False)

    # [â˜…ìˆ˜ì •] ì—¬ê¸°ì„œ CURRENT_CONTEXTë§Œ ì±„ì›Œì£¼ë©´ ë©ë‹ˆë‹¤! (SUFFICIENCY_CRITERIAëŠ” ì´ë¯¸ ë“¤ì–´ìˆìŒ)
    try:
        formatted_prompt = INTERVIEWER_PROMPT.format(
            CURRENT_CONTEXT=current_context_str
        )
    except Exception as e:
        # í˜¹ì‹œë¼ë„ í¬ë§·íŒ… ì—ëŸ¬ê°€ ë‚˜ë©´ ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
        print(f"âš ï¸ Prompt Formatting Error: {e}")
        formatted_prompt = INTERVIEWER_PROMPT.replace(
            "{{CURRENT_CONTEXT}}", "ì •ë³´ ì—†ìŒ"
        )

    messages = [SystemMessage(content=formatted_prompt)] + state["messages"]

    try:
        result = SMART_LLM.with_structured_output(InterviewResult).invoke(messages)
        new_prefs = result.user_preferences.dict(exclude_unset=True)
        updated_prefs = {
            **current_prefs,
            **{k: v for k, v in new_prefs.items() if v is not None},
        }

        if result.is_sufficient:
            print(
                f"      âœ… [Handover] ì •ë³´ í™•ë³´ ì™„ë£Œ! Researcherë¡œ ì „ë‹¬: {json.dumps(updated_prefs, ensure_ascii=False)}",
                flush=True,
            )
            return {
                "next_step": "researcher",
                "user_preferences": updated_prefs,
                "status": "ëª¨ë“  ì •ë³´ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ì²œ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤...",
                "active_mode": None,
            }

        return {
            "messages": [AIMessage(content=result.response_message)],
            "user_preferences": updated_prefs,
            "active_mode": "interviewer",
            "next_step": "end",
        }
    except Exception as e:
        print(f"Interviewer Error: {e}")
        return {"next_step": "writer"}


# ==========================================
# [REMOVED] Old researcher_node and writer_node
# These have been replaced by parallel_reco_node which consolidates
# both functionalities with FCFS streaming.
# ==========================================


async def parallel_reco_node(state: AgentState):
    member_id = state.get("member_id", 0)
    user_prefs = state.get("user_preferences", {})
    current_context = json.dumps(user_prefs, ensure_ascii=False)

    plan_llm = SMART_LLM.with_structured_output(SearchStrategyPlan)
    seen_ids = set()
    seen_ids_lock = asyncio.Lock()

    def _normalize_section_boundary(previous_text: str, next_text: str) -> str:
        if not previous_text or not next_text:
            return next_text
        if not next_text.lstrip().startswith("##"):
            return next_text
        prev_trimmed = previous_text.rstrip()
        if prev_trimmed.endswith("---") and not previous_text.endswith("\n"):
            if not next_text.startswith("\n"):
                return f"\n{next_text}"
        return next_text

    async def prepare_strategy(strategy_name: str, priority: int):
        """Phase 1: Strategy planning + search + perfume selection (parallel)"""
        print(f"   ğŸ‘‰ [Strategy {priority}] {strategy_name} ì‹œì‘", flush=True)
        
        plan_messages = [
            SystemMessage(content=RESEARCHER_SYSTEM_PROMPT),
            HumanMessage(
                content=(
                    f"ì‚¬ìš©ì ìš”ì²­ ë°ì´í„°: {current_context}\n"
                    f"ì „ëµ ì´ë¦„: {strategy_name}\n"
                    f"ìš°ì„ ìˆœìœ„: {priority}\n"
                    "ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ëµì„ ìˆ˜ë¦½í•´ ì£¼ì„¸ìš”."
                )
            ),
        ]

        try:
            plan = await plan_llm.ainvoke(
                plan_messages, config={"tags": ["internal_helper"]}
            )
        except Exception as e:
            print(f"      âŒ [Strategy {priority}] ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {e}", flush=True)
            return None

        try:
            h_filters = plan.hard_filters.model_dump(exclude_none=True)
            s_filters = plan.strategy_filters.model_dump(exclude_none=True)
        except Exception:
            h_filters = {}
            s_filters = {}

        try:
            candidates, _match_type = await smart_search_with_retry_async(
                h_filters, s_filters, query_text=plan.reason
            )
        except Exception as e:
            print(f"      âŒ [Strategy {priority}] ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True)
            return None

        selected_perfume = None
        async with seen_ids_lock:
            for candidate in candidates:
                if candidate["id"] not in seen_ids:
                    selected_perfume = candidate
                    seen_ids.add(candidate["id"])
                    break

        if not selected_perfume:
            print(f"      âŒ [Strategy {priority}] ì¤‘ë³µ ì œê±° í›„ ì„ íƒ ê°€ëŠ¥í•œ í–¥ìˆ˜ ì—†ìŒ", flush=True)
            return None
        
        print(f"      âœ… [Strategy {priority}] ì„ íƒ: {selected_perfume.get('name')} (ID: {selected_perfume.get('id')})", flush=True)

        save_recommendation_log(
            member_id=member_id, perfumes=[selected_perfume], reason=plan.reason
        )

        strategy_result = StrategyResult(
            strategy_name=plan.strategy_name,
            strategy_keyword=plan.strategy_keyword,
            strategy_reason=plan.reason,
            perfumes=[
                PerfumeDetail(
                    id=selected_perfume.get("id"),
                    perfume_name=selected_perfume.get("name"),
                    perfume_brand=selected_perfume.get("brand"),
                    accord=f"{selected_perfume.get('accords')}\n[Best Review]: {selected_perfume.get('best_review')}",
                    notes=PerfumeNotes(
                        top=selected_perfume.get("top_notes") or "N/A",
                        middle=selected_perfume.get("middle_notes") or "N/A",
                        base=selected_perfume.get("base_notes") or "N/A",
                    ),
                    image_url=selected_perfume.get("image_url"),
                    gender=selected_perfume.get("gender", "Unisex"),
                    season="All",
                    occasion="Any",
                )
            ],
        )

        section_data = {
            "user_preferences": user_prefs,
            "strategy": {
                "name": plan.strategy_name,
                "reason": plan.reason,
                "keywords": plan.strategy_keyword,
                "priority": priority,
            },
            "perfume": strategy_result.perfumes[0].dict(),
        }
        
        return {
            "section_data": section_data,
            "priority": priority,
        }

    async def generate_output(prepared_data: dict):
        """Phase 2: LLM output generation with streaming (sequential)"""
        if not prepared_data:
            return None
            
        section_data = prepared_data["section_data"]
        priority = prepared_data["priority"]
        
        USER_MODE = "BEGINNER"  # ì˜µì…˜: "BEGINNER" | "EXPERT"
        print(f"   ğŸ¥ [Strategy {priority}] ë¹„ê¸°ë„ˆìš© í”„ë¡¬í”„íŠ¸ ì ìš©", flush=True)
        
        data_ctx = json.dumps(section_data, ensure_ascii=False, indent=2)

        section_system = (
            "ë‹¹ì‹ ì€ í–¥ìˆ˜ë¥¼ ì˜ ëª¨ë¥´ëŠ” ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¸ìƒì—ì„œ ê°€ì¥ ì¹œì ˆí•˜ê³  ê°ê°ì ì¸ 'í–¥ìˆ˜ ë„ìŠ¨íŠ¸(Docent)'ì…ë‹ˆë‹¤.\n"
            "ì•„ë˜ [ì°¸ê³  ë°ì´í„°]ì— ìˆëŠ” í–¥ìˆ˜ 1ê°œë§Œ ì‚¬ìš©í•´ì„œ, 'ë‹¨ í•˜ë‚˜ì˜ ì¶”ì²œ ì„¹ì…˜'ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            "**[â˜… ZERO HALLUCINATION POLICY â˜…]**\n"
            "1. **ë°ì´í„° ê·¸ë¼ìš´ë”©(Grounding)**: ë°˜ë“œì‹œ ì œê³µëœ [ì°¸ê³  ë°ì´í„°] ë‚´ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë°ì´í„°ì— ì—†ëŠ” í–¥ìˆ˜ë‚˜ ì„±ë¶„ì„ ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.\n"
            "2. **ìˆ˜ëŸ‰ ì—„ìˆ˜**: ë°ì´í„°ê°€ 1ê°œì´ë¯€ë¡œ, ë‹¤ë¥¸ í–¥ìˆ˜ë¥¼ ì ˆëŒ€ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”. ì œê³µëœ í–¥ìˆ˜ë§Œ ì¶”ì²œí•˜ì„¸ìš”.\n\n"
            "[â˜…ì‘ì„± ê·œì¹™ - í•„ë…â˜…]\n\n"
            "1. **[ë„ì…ë¶€]**:\n"
            "   - **ì ˆëŒ€ ê¸ˆì§€**: ì „ì²´ ë„ì…ë¶€, 'ìš”ì²­í•˜ì‹  3ê°€ì§€ ì „ëµ', 'ìˆ˜ë¦½í•œ ì „ëµëŒ€ë¡œ' ê°™ì€ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€.\n"
            f"   - ë°”ë¡œ '## {priority}.' ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n\n"
            "2. **[ì¶”ì²œ ì´ìœ  (Logical Connection) - â˜…ì´ë¯¸ì§€ ì „ëµ í•©ì„±]**:\n"
            "   - **ë¸Œë¦¿ì§€(Bridge) ì„¤ëª…**: **ì‚¬ìš©ìì˜ ì •ë³´(ëŒ€ìƒ, ê³„ì ˆ ë“±)**ì™€ **ë¦¬ì„œì²˜ê°€ ì „ë‹¬í•œ ì „ëµì  ì˜ë„(strategy_reason)**ë¥¼ í•˜ë‚˜ì˜ ìœ ê¸°ì ì¸ ë¬¸ì¥ìœ¼ë¡œ í•©ì³ì„œ ì„¤ëª…í•˜ì„¸ìš”.\n"
            "   - **í•µì‹¬**: ë¦¬ì„œì²˜ì˜ ë¶„ì„ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì½ì–´ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë„ìŠ¨íŠ¸ë¡œì„œ ì‚¬ìš©ìì—ê²Œ ë§ì„ ê±´ë„¤ëŠ” ë”°ëœ»í•œ ë¬¸íˆ¬ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.\n"
            "   - **ì¼ìƒì–´ ë³€í™˜**: 'ìš°ë””', 'ìŠ¤íŒŒì´ì‹œ', 'ë¨¸ìŠ¤í¬' ë“± ì „ë¬¸ ìš©ì–´ë¥¼ ì§ì ‘ ì“°ì§€ ë§ê³  ëŠë‚Œìœ¼ë¡œ í’€ì–´ì„œ ì „ë‹¬í•˜ì„¸ìš”.\n\n"
            "3. **[í–¥ ë¬˜ì‚¬ ë° ìš©ì–´]**:\n"
            "   - **ì¼ìƒì–´ ë³€í™˜**: 'íƒ‘/ë¯¸ë“¤/ë² ì´ìŠ¤' ìš©ì–´ ê¸ˆì§€. 'ê½ƒì§‘ì— ë“¤ì–´ì„  ë“¯í•œ í–¥', 'í¬ê·¼í•œ ì‚´ëƒ„ìƒˆ'ì²˜ëŸ¼ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.\n"
            "   - **ì„±ë³„ í‘œí˜„**: 'ìœ ë‹ˆì„¹ìŠ¤' ëŒ€ì‹  'ë‚¨ì„±ì—ê²Œ, ì—¬ì„±ì—ê²Œ, ë‚¨ë…€ ëª¨ë‘ì—ê²Œ ì˜ ì–´ìš¸ë ¤ìš”' í˜•íƒœë¡œ í‘œí˜„í•˜ì„¸ìš”.\n\n"
            "4. **[í˜•ì‹ ê·œì¹™]**:\n"
            f"   - ì¶œë ¥ì€ ë°˜ë“œì‹œ '## {priority}.' ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n"
            "   - ì„¹ì…˜ ì•ˆì— '---' êµ¬ë¶„ì„ ì„ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš” (êµ¬ë¶„ì„ ì€ ë°”ê¹¥ì—ì„œ ë¶™ì…ë‹ˆë‹¤).\n"
            "   - ë°˜ë“œì‹œ ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ì„ í¬í•¨í•˜ì„¸ìš”: ![ì´ë¦„](ì´ë¯¸ì§€ë§í¬)\n"
            "   - ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ ì €ì¥ íƒœê·¸ë¥¼ í¬í•¨í•˜ì„¸ìš”: [[SAVE:í–¥ìˆ˜ID:í–¥ìˆ˜ëª…]]\n\n"
            f"[ëª¨ë“œ]\n"
            f"- USER_MODE={USER_MODE}\n"
            "- ìì—°ìŠ¤ëŸ½ê³  ì‚°ëœ»í•œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
            "[ì°¸ê³  ë°ì´í„°]:\n"
            f"{data_ctx}"
        )

        messages = [SystemMessage(content=section_system)] + state["messages"]

        try:
            response = await SUPER_SMART_LLM.ainvoke(messages)
            print(f"      âœ… [Strategy {priority}] ì‘ì„± ì™„ë£Œ ({len(response.content)} chars)", flush=True)
            return response.content
        except Exception as e:
            print(f"      âŒ [Strategy {priority}] ì‘ì„± ì‹¤íŒ¨: {e}", flush=True)
            return None

    # Phase 1: Parallel preparation (strategy planning + search)
    # All 3 strategies run simultaneously - fast!
    prep_tasks = [
        asyncio.create_task(prepare_strategy("ì´ë¯¸ì§€ ê°•ì¡°", 1)),
        asyncio.create_task(prepare_strategy("ì´ë¯¸ì§€ ë³´ì™„", 2)),
        asyncio.create_task(prepare_strategy("ì´ë¯¸ì§€ ë°˜ì „", 3)),
    ]
    
    # Phase 2: Sequential output generation with streaming
    # Wait for prep in order, then generate output with streaming
    results = []
    try:
        data1 = await prep_tasks[0]
        result1 = await generate_output(data1) if data1 else None
        results.append(result1)
        
        data2 = await prep_tasks[1]
        result2 = await generate_output(data2) if data2 else None
        results.append(result2)
        
        data3 = await prep_tasks[2]
        result3 = await generate_output(data3) if data3 else None
        results.append(result3)
    except (Exception, asyncio.CancelledError) as e:
        print(f"   âš ï¸ [Parallel Reco] Task cancelled or failed: {e}", flush=True)
        return {
            "messages": [AIMessage(content="ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜¢")],
            "next_step": "end",
        }

    # Assemble sections in order (1 â†’ 2 â†’ 3)
    full_text = ""
    for idx, result_text in enumerate(results, start=1):
        # Handle exceptions returned by gather(return_exceptions=True)
        if isinstance(result_text, (Exception, asyncio.CancelledError)):
            print(f"   âš ï¸ [Section {idx}] Failed: {result_text}", flush=True)
            continue

        if not result_text:
            continue

        if full_text:
            # Add separator with clean boundaries
            full_text = f"{full_text}\n\n---\n\n{result_text}"
        else:
            full_text = result_text

    if not full_text:
        full_text = "ì¡°ê±´ì— ë§ëŠ” í–¥ìˆ˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ğŸ˜¢ ëŒ€ì•ˆì„ ì•ˆë‚´í•´ ë“œë¦´ê²Œìš”..."

    return {"messages": [AIMessage(content=full_text)], "next_step": "end"}


# ==========================================
# 4. Graph Build
# ==========================================
workflow = StateGraph(AgentState)

workflow.add_node("supervisor", supervisor_node)
workflow.add_node("interviewer", interviewer_node)
# workflow.add_node("researcher", researcher_node)  # Replaced by parallel_reco
# workflow.add_node("writer", writer_node)  # Replaced by parallel_reco
workflow.add_node("parallel_reco", parallel_reco_node)
workflow.add_node("info_retrieval_subgraph", call_info_graph_wrapper)

workflow.add_edge(START, "supervisor")

workflow.add_conditional_edges(
    "supervisor",
    lambda x: x["next_step"],
    {
        "interviewer": "interviewer",
        "info_retrieval": "info_retrieval_subgraph",
        "writer": "parallel_reco",  # Replaced writer with parallel_reco
    },
)

workflow.add_conditional_edges(
    "interviewer",
    lambda x: x["next_step"],
    {"end": END, "researcher": "parallel_reco", "writer": "parallel_reco"},
)

# workflow.add_edge("researcher", "writer")  # Old flow - replaced
# workflow.add_edge("writer", END)  # Old flow - replaced
workflow.add_edge("parallel_reco", END)
workflow.add_edge("info_retrieval_subgraph", END)

checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
